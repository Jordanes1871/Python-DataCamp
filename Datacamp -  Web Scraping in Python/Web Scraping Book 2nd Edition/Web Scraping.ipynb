{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Python - Collecting More Data from the Modern Web - \n",
    "https://edu.anarcho-copy.org/Programming%20Languages/Python/Web%20Scraping%20with%20Python,%202nd%20Edition.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Your First Web Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7712c9e5",
   "metadata": {},
   "source": [
    "urllib = is a standard Python library (meaning you don’t have to install anything extra\n",
    "to run this example) and contains functions for requesting data across the web, handling cookies, and even changing metadata such as headers and your user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa703694",
   "metadata": {},
   "source": [
    "urlopen = used to open a remote object across a network and read it. Because it is a fairly generic function (it can read HTML files, image files, or any other file stream with ease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install & run beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')\n",
    "#beautifulsoup object with 2 arguements, html text and parser you want to use to create object \n",
    "bs = BeautifulSoup(html.read(), 'html.parser') \n",
    "print(bs.h1) #we are running the h1 tag found on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e5954",
   "metadata": {},
   "source": [
    "equivilant code that will write the title...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.html.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.html.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h1>An Interesting Title</h1>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### another popular parser which has the benefit of parsing messy or malformed HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (4.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf37c5",
   "metadata": {},
   "source": [
    "lxml has some advantages over html.parser in that it is generally better at parsing\n",
    "“messy” or malformed HTML code. It is forgiving and fixes problems like unclosed\n",
    "tags, tags that are improperly nested, and missing head or body tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(html.read(), 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular HTML parser is html5lib. Like lxml, html5lib is an extremely forgiving parser that takes even more initiative correcting broken HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/jordanstevens/anaconda3/lib/python3.11/site-packages (from html5lib) (0.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(html.read(), 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html5lib is an extremely forgiving parser that takes even more initiative correcting broken HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.html.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting Reliability and Handling Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The web is messy. Data is poorly formatted, websites go down, and closing tags go\n",
    "missing. One of the most frustrating experiences in web scraping is to go to sleep\n",
    "with a scraper running, dreaming of all the data you’ll have in your database the next\n",
    "day—only to find that the scraper hit an error on some unexpected data format and\n",
    "stopped execution shortly after you stopped looking at the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen('http://www.pythonscraping.com/pages/page1.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an HTTP error code is returned, the program now prints the error, and does not\n",
    "execute the rest of the program under the else statement.\n",
    "\n",
    "If the server is not found at all (if, say, http://www.pythonscraping.com is down, or the\n",
    "URL is mis-typed), urlopen will throw an URLError\n",
    "\n",
    "You can add a check to see whether this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server could not be found!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "try:\n",
    "    html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e: #some other plan\n",
    "    print('The server could not be found!')\n",
    "else:\n",
    "    print('It Worked!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if the page is retrieved successfully from the server, there is still the issue of\n",
    "the content on the page not quite being what you expected. Every time you access a\n",
    "tag in a BeautifulSoup object, it’s smart to add a check to make sure the tag actually\n",
    "exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/63qltpj12zq4ms95z852hkg40000gn/T/ipykernel_6117/2252819537.py:2: DeprecationWarning: .nonExistentTag is deprecated, use .find(\"nonExistent\") instead. If you really were looking for a tag called nonExistentTag, use .find(\"nonExistentTag\")\n",
      "  print(bs.nonExistentTag)\n"
     ]
    }
   ],
   "source": [
    "# The line (where nonExistentTag is a made-up tag, not the name of a realBeautifulSoup function)\n",
    "print(bs.nonExistentTag) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/63qltpj12zq4ms95z852hkg40000gn/T/ipykernel_6117/2402693011.py:1: DeprecationWarning: .nonExistentTag is deprecated, use .find(\"nonExistent\") instead. If you really were looking for a tag called nonExistentTag, use .find(\"nonExistentTag\")\n",
      "  print(bs.nonExistentTag.someTag)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'someTag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(bs\u001b[39m.\u001b[39mnonExistentTag\u001b[39m.\u001b[39msomeTag)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'someTag'"
     ]
    }
   ],
   "source": [
    "print(bs.nonExistentTag.someTag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can you guard against these two situations? The easiest way is to explicitly\n",
    "check for both situations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag was not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/63qltpj12zq4ms95z852hkg40000gn/T/ipykernel_6117/2524236393.py:2: DeprecationWarning: .nonExistingTag is deprecated, use .find(\"nonExisting\") instead. If you really were looking for a tag called nonExistingTag, use .find(\"nonExistingTag\")\n",
      "  badContent = bs.nonExistingTag.anotherTag\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    badContent = bs.nonExistingTag.anotherTag\n",
    "except AttributeError as e:\n",
    "    print('Tag was not found')\n",
    "else:\n",
    "    if badContent == None:\n",
    "        print('Tag was not found')\n",
    "    else:\n",
    "        print(badContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2659d",
   "metadata": {},
   "source": [
    "## Connecting Reliably and handling Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This checking and handling of every error does seem laborious at first, but it’s easy to\n",
    "add a little reorganization to this code to make it less difficult to write. This code, for example, is our same scraper\n",
    "written in a slightly different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url): \n",
    " try:\n",
    "     html = urlopen(url)\n",
    " except HTTPError as e:\n",
    "     return None\n",
    " try:\n",
    "     bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "     title = bs.body.h1 \n",
    " except AttributeError as e:\n",
    "     return None\n",
    " return title\n",
    "\n",
    "title = getTitle('http://www.pythonscraping.com/pages/page1.html')\n",
    "if title == None:\n",
    "     print('Title could not be found')\n",
    "else:\n",
    "     print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you’re creating a function getTitle, which returns either the title of\n",
    "the page, or a None object if there was a problem retrieving it. Inside getTitle, you\n",
    "check for an HTTPError, as in the previous example, and encapsulate two of the Beau‐\n",
    "tifulSoup lines inside one try statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Advanced HTML Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say you have some target content. Maybe it’s a name, statistic, or block of text.\n",
    "Maybe it’s buried 20 tags deep in an HTML mush with no helpful tags or HTML\n",
    "attributes to be found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look for a “Print This Page” link, or perhaps a mobile version of the site that has\n",
    "better-formatted HTML.\n",
    "\n",
    "2. Look for the information hidden in a JavaScript file. Remember, you might need\n",
    "to examine the imported JavaScript files in order to do this.\n",
    "\n",
    "3. This is more common for page titles, but the information might be available in\n",
    "the URL of the page itself.\n",
    "\n",
    "4. If the information you are looking for is unique to this website for some reason,\n",
    "you’re out of luck. If not, try to think of other sources you could get this information from. Is there another website with the same data? Is this website displaying\n",
    "data that it scraped or aggregated from another website?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS\n",
    "relies on the differentiation of HTML elements that might otherwise have the exact\n",
    "same markup in order to style them differently. Some tags might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<span class=\"green\"></span>\\n<span class=\"red\"></span>\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<span class=\"green\"></span>\n",
    "<span class=\"red\"></span>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scrapers can easily separate these two tags based on their class; for example, they\n",
    "might use BeautifulSoup to grab all the red text but none of the green text. Because\n",
    "CSS relies on these identifying attributes to style sites appropriately, you are almost\n",
    "guaranteed that these class and ID attributes will be plentiful on most modern web‐\n",
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(' http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>War and Peace</h1>\n"
     ]
    }
   ],
   "source": [
    "# retrieve the title\n",
    "print(bs.html.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b5416",
   "metadata": {},
   "source": [
    "## Find() and find_all() with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, you’ve called bs.tagName to get the first occurrence of that tag on the page. Now, you’re calling bs.find_all(tagName, tagAttributes) to get a list of all of the tags on the page, rather than just the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "# define variable that finds all the green class types\n",
    "nameList = bs.findAll('span', {'class':'green'})\n",
    "\n",
    "for name in nameList:\n",
    "    print(name.get_text())  # returns all the green text in the order they appear in war and peace\n",
    "                            # this can be checked by typing in the url on the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup’s find() and find_all() are the two functions you will likely use the\n",
    "most. With them, you can easily filter HTML pages to find lists of desired tags, or a\n",
    "single tag, based on their various attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find_all(tag, attributes, recursive, text, limit, keywords)\n",
    "\n",
    "find(tag, attributes, recursive, text, keywords)\n",
    "\n",
    "In all likelihood, 95% of the time you will need to use only the first two arguments:\n",
    "tag and attributes. However, let’s take a look at all the arguments in greater detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1>War and Peace</h1>, <h2>Chapter 1</h2>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lists all headers in the doc\n",
    "bs.find_all(['h1','h2','h3','h4','h5','h6']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"red\">Well, Prince, so Genoa and Lucca are now just family estates of the\n",
       " Buonapartes. But I warn you, if you don't tell me that this means war,\n",
       " if you still try to defend the infamies and horrors perpetrated by\n",
       " that Antichrist- I really believe he is Antichrist- I will have\n",
       " nothing more to do with you and you are no longer my friend, no longer\n",
       " my 'faithful slave,' as you call yourself! But how do you do? I see\n",
       " I have frightened you- sit down and tell me all the news.</span>,\n",
       " <span class=\"green\">Anna\n",
       " Pavlovna Scherer</span>,\n",
       " <span class=\"green\">Empress Marya\n",
       " Fedorovna</span>,\n",
       " <span class=\"green\">Prince Vasili Kuragin</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"green\">St. Petersburg</span>,\n",
       " <span class=\"red\">If you have nothing better to do, Count [or Prince], and if the\n",
       " prospect of spending an evening with a poor invalid is not too\n",
       " terrible, I shall be very charmed to see you tonight between 7 and 10-\n",
       " Annette Scherer.</span>,\n",
       " <span class=\"red\">Heavens! what a virulent attack!</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"red\">First of all, dear friend, tell me how you are. Set your friend's\n",
       " mind at rest,</span>,\n",
       " <span class=\"red\">Can one be well while suffering morally? Can one be calm in times\n",
       " like these if one has any feeling?</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"red\">You are\n",
       " staying the whole evening, I hope?</span>,\n",
       " <span class=\"red\">And the fete at the English ambassador's? Today is Wednesday. I\n",
       " must put in an appearance there,</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">My daughter is\n",
       " coming for me to take me there.</span>,\n",
       " <span class=\"red\">I thought today's fete had been canceled. I confess all these\n",
       " festivities and fireworks are becoming wearisome.</span>,\n",
       " <span class=\"red\">If they had known that you wished it, the entertainment would\n",
       " have been put off,</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">Don't tease! Well, and what has been decided about Novosiltsev's\n",
       " dispatch? You know everything.</span>,\n",
       " <span class=\"red\">What can one say about it?</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">What has been decided? They have decided that\n",
       " Buonaparte has burnt his boats, and I believe that we are ready to\n",
       " burn ours.</span>,\n",
       " <span class=\"green\">Prince Vasili</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"red\">Oh, don't speak to me of Austria. Perhaps I don't understand\n",
       " things, but Austria never has wished, and does not wish, for war.\n",
       " She is betraying us! Russia alone must save Europe. Our gracious\n",
       " sovereign recognizes his high vocation and will be true to it. That is\n",
       " the one thing I have faith in! Our good and wonderful sovereign has to\n",
       " perform the noblest role on earth, and he is so virtuous and noble\n",
       " that God will not forsake him. He will fulfill his vocation and\n",
       " crush the hydra of revolution, which has become more terrible than\n",
       " ever in the person of this murderer and villain! We alone must\n",
       " avenge the blood of the just one.... Whom, I ask you, can we rely\n",
       " on?... England with her commercial spirit will not and cannot\n",
       " understand the Emperor Alexander's loftiness of soul. She has\n",
       " refused to evacuate Malta. She wanted to find, and still seeks, some\n",
       " secret motive in our actions. What answer did Novosiltsev get? None.\n",
       " The English have not understood and cannot understand the\n",
       " self-abnegation of our Emperor who wants nothing for himself, but only\n",
       " desires the good of mankind. And what have they promised? Nothing! And\n",
       " what little they have promised they will not perform! Prussia has\n",
       " always declared that Buonaparte is invincible, and that all Europe\n",
       " is powerless before him.... And I don't believe a word that Hardenburg\n",
       " says, or Haugwitz either. This famous Prussian neutrality is just a\n",
       " trap. I have faith only in God and the lofty destiny of our adored\n",
       " monarch. He will save Europe!</span>,\n",
       " <span class=\"red\">I think,</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">that if you had been\n",
       " sent instead of our dear <span class=\"green\">Wintzingerode</span> you would have captured the\n",
       " <span class=\"green\">King of Prussia</span>'s consent by assault. You are so eloquent. Will you\n",
       " give me a cup of tea?</span>,\n",
       " <span class=\"green\">Wintzingerode</span>,\n",
       " <span class=\"green\">King of Prussia</span>,\n",
       " <span class=\"red\">In a moment. A propos,</span>,\n",
       " <span class=\"red\">I am\n",
       " expecting two very interesting men tonight, <span class=\"green\">le Vicomte de Mortemart</span>,\n",
       " who is connected with the <span class=\"green\">Montmorencys</span> through the <span class=\"green\">Rohans</span>, one of\n",
       " the best French families. He is one of the genuine emigres, the good\n",
       " ones. And also the <span class=\"green\">Abbe Morio</span>. Do you know that profound thinker? He\n",
       " has been received by <span class=\"green\">the Emperor</span>. Had you heard?</span>,\n",
       " <span class=\"green\">le Vicomte de Mortemart</span>,\n",
       " <span class=\"green\">Montmorencys</span>,\n",
       " <span class=\"green\">Rohans</span>,\n",
       " <span class=\"green\">Abbe Morio</span>,\n",
       " <span class=\"green\">the Emperor</span>,\n",
       " <span class=\"red\">I shall be delighted to meet them,</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">But tell me,</span>,\n",
       " <span class=\"red\">is it true that the Dowager Empress wants Baron Funke\n",
       " to be appointed first secretary at Vienna? The baron by all accounts\n",
       " is a poor creature.</span>,\n",
       " <span class=\"green\">Prince Vasili</span>,\n",
       " <span class=\"green\">Dowager Empress Marya Fedorovna</span>,\n",
       " <span class=\"green\">the baron</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>,\n",
       " <span class=\"green\">the Empress</span>,\n",
       " <span class=\"red\">Baron Funke has been recommended to the Dowager Empress by her\n",
       " sister,</span>,\n",
       " <span class=\"green\">the Empress</span>,\n",
       " <span class=\"green\">Anna Pavlovna's</span>,\n",
       " <span class=\"green\">Her Majesty</span>,\n",
       " <span class=\"green\">Baron\n",
       " Funke</span>,\n",
       " <span class=\"green\">The prince</span>,\n",
       " <span class=\"green\">Anna\n",
       " Pavlovna</span>,\n",
       " <span class=\"green\">the Empress</span>,\n",
       " <span class=\"red\">Now about your family. Do you know that since your daughter came\n",
       " out everyone has been enraptured by her? They say she is amazingly\n",
       " beautiful.</span>,\n",
       " <span class=\"green\">The prince</span>,\n",
       " <span class=\"red\">I often think,</span>,\n",
       " <span class=\"red\">I often think how unfairly sometimes the\n",
       " joys of life are distributed. Why has fate given you two such splendid\n",
       " children? I don't speak of <span class=\"green\">Anatole</span>, your youngest. I don't like\n",
       " him,</span>,\n",
       " <span class=\"green\">Anatole</span>,\n",
       " <span class=\"red\">Two such charming children. And really you appreciate\n",
       " them less than anyone, and so you don't deserve to have them.</span>,\n",
       " <span class=\"red\">I can't help it,</span>,\n",
       " <span class=\"green\">the prince</span>,\n",
       " <span class=\"red\">Lavater would have said I\n",
       " lack the bump of paternity.</span>,\n",
       " <span class=\"red\">Don't joke; I mean to have a serious talk with you. Do you know I\n",
       " am dissatisfied with your younger son? Between ourselves</span>,\n",
       " <span class=\"red\">he was mentioned at Her\n",
       " Majesty's and you were pitied....</span>,\n",
       " <span class=\"green\">The prince</span>,\n",
       " <span class=\"red\">What would you have me do?</span>,\n",
       " <span class=\"red\">You know I did all\n",
       " a father could for their education, and they have both turned out\n",
       " fools. Hippolyte is at least a quiet fool, but Anatole is an active\n",
       " one. That is the only difference between them.</span>,\n",
       " <span class=\"red\">And why are children born to such men as you? If you were not a\n",
       " father there would be nothing I could reproach you with,</span>,\n",
       " <span class=\"green\">Anna\n",
       " Pavlovna</span>,\n",
       " <span class=\"red\">I am your faithful slave and to you alone I can confess that my\n",
       " children are the bane of my life. It is the cross I have to bear. That\n",
       " is how I explain it to myself. It can't be helped!</span>,\n",
       " <span class=\"green\">Anna Pavlovna</span>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns green and red span tags in html doc\n",
    "bs.find_all('span', {'class':{'green', 'red'}}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text argument is unusual in that it matches based on the text content of the tags,rather than properties of the tags themselves. For instance, if you want to find the number of times “the prince” is surrounded by tags on the example page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/63qltpj12zq4ms95z852hkg40000gn/T/ipykernel_6117/2105580664.py:2: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  nameList = bs.find_all(text='the prince')\n"
     ]
    }
   ],
   "source": [
    "# if you want to find the number of times “the prince” is surrounded by tags on the example page\n",
    "nameList = bs.find_all(text='the prince')\n",
    "print(len(nameList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The keyword argument allows you to select tags that contain a particular attribute or set of attributes. For example:\n",
    "title = bs.find_all(id='title', class_='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recap\n",
    "\n",
    "BeautifulSoup objects = Instances seen in previous code examples as the variable bs\n",
    "\n",
    "Tag objects = Retrieved in lists, or retrieved individually by calling find and find_all on a BeautifulSoup object, or drilling down, as follows: = bs.div.h1\n",
    "\n",
    "However, there are two more objects in the library that, although less commonly used, are still important to know about:\n",
    "NavigableString objects = Used to represent text within tags, rather than the tags themselves (some functions operate on and produce NavigableStrings, rather than tag objects).\n",
    "\n",
    "Comment object = Used to find HTML comments in comment tags, <!--like this one-->.\n",
    "\n",
    "These four objects are the only objects you will ever encounter in the BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.pythonscraping.com/pages/page3.html ---- shopping example\n",
    "        \n",
    "which has a tree like HTML structure:\n",
    "    \n",
    "• HTML\n",
    "— body\n",
    "— div.wrapper\n",
    "— h1\n",
    "— div.content\n",
    "— table#giftList\n",
    "— tr\n",
    "— th\n",
    "— th\n",
    "— th\n",
    "— th\n",
    "— tr.gift#gift1\n",
    "— td\n",
    "— td\n",
    "— span.excitingNote\n",
    "— td\n",
    "— td\n",
    "— img\n",
    "— ...table rows continue...\n",
    "— div.footer\n",
    "\n",
    "In the BeautifulSoup library, as well as many other libraries, there is a distinction\n",
    "drawn between children and descendants: much like in a human family tree, children\n",
    "are always exactly one tag below a parent, whereas descendants can be at any level in\n",
    "the tree below a parent. For example, the tr tags are children of the table tag,\n",
    "whereas tr, th, td, img, and span are all descendants of the table tag (at least in our\n",
    "example page). All children are descendants, but not all descendants are children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr><th>\n",
      "Item Title\n",
      "</th><th>\n",
      "Description\n",
      "</th><th>\n",
      "Cost\n",
      "</th><th>\n",
      "Image\n",
      "</th></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#If you want to find only descendants that are children, you can use the .children tag:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for child in bs.find('table',{'id':'giftList'}).children:\n",
    "    print(child)\n",
    "\n",
    "# This code prints the list of product rows in the giftList table, including the initial\n",
    "# row of column labels. If you were to write it using the descendants() function\n",
    "# instead of the children() function, about two dozen tags would be found within the\n",
    "# table and printed, including img tags, span tags, and individual td tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with siblings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124a316",
   "metadata": {},
   "source": [
    "\n",
    "The output of this code is to print all rows of products from the product table, except\n",
    "for the first title row. Why does the title row get skipped? Objects cannot be siblings\n",
    "with themselves. Anytime you get siblings of an object, the object itself will not be\n",
    "included in the list. As the name of the function implies, it calls next siblings only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift1\"><td>\n",
      "Vegetable Basket\n",
      "</td><td>\n",
      "This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
      "<span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
      "</td><td>\n",
      "$15.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img1.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift2\"><td>\n",
      "Russian Nesting Dolls\n",
      "</td><td>\n",
      "Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
      "</td><td>\n",
      "$10,000.52\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img2.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift3\"><td>\n",
      "Fish Painting\n",
      "</td><td>\n",
      "If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
      "</td><td>\n",
      "$10,005.00\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img3.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift4\"><td>\n",
      "Dead Parrot\n",
      "</td><td>\n",
      "This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
      "</td><td>\n",
      "$0.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img4.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n",
      "<tr class=\"gift\" id=\"gift5\"><td>\n",
      "Mystery Box\n",
      "</td><td>\n",
      "If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
      "</td><td>\n",
      "$1.50\n",
      "</td><td>\n",
      "<img src=\"../img/gifts/img6.jpg\"/>\n",
      "</td></tr>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The BeautifulSoup next_siblings() function makes it trivial to collect data from tables, especially ones with title rows:\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping pages, you will likely discover that you need to find parents of tags less frequently than you need to find their children or siblings. \n",
    "\n",
    "Typically, when you look at HTML pages with the goal of crawling them, you start by looking at the top layer of tags, and then figure out how to drill your way down into the exact piece of data that you want. \n",
    "\n",
    "Occasionally, however, you can find yourself in odd situations that require BeautifulSoup’s parent-finding functions, .parent and .parents. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "$15.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "print(bs.find('img',{'src':'../img/gifts/img1.jpg'}).parent.previous_sibling.get_text())\n",
    "\n",
    "#prev sibling of the td tag is the td tag that contains the dollar value of the product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "\n",
    "https://www.pythonscraping.com/pages/page3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice that the site has many product images which takes the following form:>>>\"../img/gifts/img3.jpg\">\n",
    "\n",
    "If you wanted to grab URLs to all of the product images, it might seem fairly straight‐forward at first: just grab all the image tags by using.find_all(\"img\"), right?\n",
    "\n",
    "But here’s a problem. In addition to the obvious “extra” images (e.g., logos), modern web‐sites often have hidden images, blank images used for spacing and aligning elements, ad other random image tags you might not be aware of. Certainly, you can’t count n the only images on the page being product images.\n",
    "\n",
    "Let’s also assume that the layout of the page might change, or that, for whatever reason, you don’t want to depend on the position of the image in the page in order to ind the correct tag.\n",
    "\n",
    "The solution is to look for something identifying about the tag itself. In this case, you\n",
    "can look at the file path of the product images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../img/gifts/img1.jpg\n",
      "../img/gifts/img2.jpg\n",
      "../img/gifts/img3.jpg\n",
      "../img/gifts/img4.jpg\n",
      "../img/gifts/img6.jpg\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen('http://www.pythonscraping.com/pages/page3.html')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "images = bs.find_all('img',{'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "for image in images:\n",
    "    print(image['src'])\n",
    "    \n",
    "#prints relative image paths that start with /img/giftd/img and end in jpg - * says all i.e. img.* referring to 1/2/3 etc..    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accessing attributes\n",
    "\n",
    "With tag objects, a Python list of attributes can be automatically accessed by calling this: \n",
    "\n",
    "myTag.attrs\n",
    "\n",
    "The source location for animage, for example, can be found using the following line:\n",
    "\n",
    "myImgTag.attrs['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Expressions\n",
    "\n",
    "Is a function that is passed into another function as a variable; instead of defining a function as f(x, y), you may define a function as f(g(x),y) or even f(g(x), h(x)).\n",
    "\n",
    "BeautifulSoup allows you to pass certain types of functions as parameters into the find_all function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only restriction is that these functions must take a tag object as an argument and return a boolean. \n",
    "\n",
    "Every tag object that BeautifulSoup encounters is evaluated in this function, and tags that evaluate to True are returned, while the rest are discarded.\n",
    "\n",
    "For example, the following retrieves all tags that have exactly two attributes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"../img/gifts/logo.jpg\" style=\"float:left;\"/>,\n",
       " <tr class=\"gift\" id=\"gift1\"><td>\n",
       " Vegetable Basket\n",
       " </td><td>\n",
       " This vegetable basket is the perfect gift for your health conscious (or overweight) friends!\n",
       " <span class=\"excitingNote\">Now with super-colorful bell peppers!</span>\n",
       " </td><td>\n",
       " $15.00\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img1.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift2\"><td>\n",
       " Russian Nesting Dolls\n",
       " </td><td>\n",
       " Hand-painted by trained monkeys, these exquisite dolls are priceless! And by \"priceless,\" we mean \"extremely expensive\"! <span class=\"excitingNote\">8 entire dolls per set! Octuple the presents!</span>\n",
       " </td><td>\n",
       " $10,000.52\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img2.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift3\"><td>\n",
       " Fish Painting\n",
       " </td><td>\n",
       " If something seems fishy about this painting, it's because it's a fish! <span class=\"excitingNote\">Also hand-painted by trained monkeys!</span>\n",
       " </td><td>\n",
       " $10,005.00\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img3.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift4\"><td>\n",
       " Dead Parrot\n",
       " </td><td>\n",
       " This is an ex-parrot! <span class=\"excitingNote\">Or maybe he's only resting?</span>\n",
       " </td><td>\n",
       " $0.50\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img4.jpg\"/>\n",
       " </td></tr>,\n",
       " <tr class=\"gift\" id=\"gift5\"><td>\n",
       " Mystery Box\n",
       " </td><td>\n",
       " If you love suprises, this mystery box is for you! Do not place on light-colored surfaces. May cause oil staining. <span class=\"excitingNote\">Keep your friends guessing!</span>\n",
       " </td><td>\n",
       " $1.50\n",
       " </td><td>\n",
       " <img src=\"../img/gifts/img6.jpg\"/>\n",
       " </td></tr>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs.find_all(lambda tag: len(tag.attrs) == 2)\n",
    "\n",
    "#Here, the function that you are passing as the argument is len(tag.attrs) == 2.\n",
    "#Where this is True, the find_all function will return the tag. That is, it will find tags\n",
    "# with two attributes, such as the following:\n",
    "# <div class=\"body\" id=\"content\"></div>\n",
    "# <span style=\"color:red\" class=\"title\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"excitingNote\">Or maybe he's only resting?</span>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lambda functions are so useful you can even use them to replace existing Beauti‐\n",
    "#fulSoup functions:    \n",
    "bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h1/63qltpj12zq4ms95z852hkg40000gn/T/ipykernel_6117/3869743755.py:2: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  bs.find_all('', text='Or maybe he\\'s only resting?')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Or maybe he's only resting?\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This can also be accomplished without a lambda function:\n",
    "bs.find_all('', text='Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "1. Write the letter a at least once.\n",
    "2. Append to this the letter b exactly five times.\n",
    "3. Append to this the letter c any even number of times.\n",
    "4. Write either the letter d or e at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsolution = aa*bbbbb(cc)*(d|e)\\n\\naa* = any number of as a inc 0 of them (written at least once)\\nbbbbb = 5 b's in a row\\n(cc)* = any even number or things that can be grouped into pairs\\n(d/e) = d or e\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "solution = aa*bbbbb(cc)*(d|e)\n",
    "\n",
    "aa* = any number of as a inc 0 of them (written at least once)\n",
    "bbbbb = 5 b's in a row\n",
    "(cc)* = any even number or things that can be grouped into pairs\n",
    "(d/e) = d or e\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for testing regular expressions\n",
    "\n",
    "https://www.regexpal.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00f7de87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n* = matches preceding char 0 or more times i.e. a*b* = aaaaaaaaa,aaabbbbbbbb, bbbbbb\\n\"+\" = matches the preceding character 1 or more times i.e a+b+ = aaaaab, aaabbbbb, abbbbb\\n[] = matches any character wihtin the brackets i..e. [A-Z]* = APPLE, CAPITALS, QUERY\\n() = grouped sub expression i.e. (a*b)* = aaabaaab,  abaaab, ababaaab\\n(m,n) = matches the preceding character between m and n inclusive i.e. a(2,3)b(2,3) = aabbb, aaabbb, aabb\\n[^] = matches any single character that is not in the brackets i.e. [^A-Z]* = apple, lowercase, qwerty\\n| = matches any character/string seperated by | i.e. b(a|i|e|d) = bad, bid, bed\\n. = matches any single char at beggining of string i.e. b.d = bad, bzd, b$d, b d\\n^ = indicates that a character occurs at begginng of string i.e. ^a = aple, asdf, a \\n\\\\ = escape character allows special char as literal meanings i.e. \\\\.\\\\|\\\\ = .|$ = often used at end of regular expression match this up to end of string i.e. [A-Z]*[a-z]*$ = ABCabc, zzzyx, Bob\\n?! = does not contain i.e. ^((?![A-Z]).)*$ = co-caps-here, $ymb0ls a4e f!ne\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "* = matches preceding char 0 or more times i.e. a*b* = aaaaaaaaa,aaabbbbbbbb, bbbbbb\n",
    "\"+\" = matches the preceding character 1 or more times i.e a+b+ = aaaaab, aaabbbbb, abbbbb\n",
    "[] = matches any character wihtin the brackets i..e. [A-Z]* = APPLE, CAPITALS, QUERY\n",
    "() = grouped sub expression i.e. (a*b)* = aaabaaab,  abaaab, ababaaab\n",
    "(m,n) = matches the preceding character between m and n inclusive i.e. a(2,3)b(2,3) = aabbb, aaabbb, aabb\n",
    "[^] = matches any single character that is not in the brackets i.e. [^A-Z]* = apple, lowercase, qwerty\n",
    "| = matches any character/string seperated by | i.e. b(a|i|e|d) = bad, bid, bed\n",
    ". = matches any single char at beggining of string i.e. b.d = bad, bzd, b$d, b d\n",
    "^ = indicates that a character occurs at begginng of string i.e. ^a = aple, asdf, a \n",
    "\\ = escape character allows special char as literal meanings i.e. \\.\\|\\\\ = .|\\\n",
    "$ = often used at end of regular expression match this up to end of string i.e. [A-Z]*[a-z]*$ = ABCabc, zzzyx, Bob\n",
    "?! = does not contain i.e. ^((?![A-Z]).)*$ = co-caps-here, $ymb0ls a4e f!ne\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEMAIL ADDRESS EXAMPLE\\n\\n[A-Za-z0-9\\\\._+]+@[A-Za-z]+\\\\.(com|org|edu|net)\\n\\nfirst part contains at least 1 upper/lower case letters, numbers 0-9, periods(.), plus signs (+), or underscores (_)\\nnext contains @\\nthen contians at least 1 upper/lower case letter\\nfollowed by period (.)\\nfollowed by .com/edu/net etc\\n '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "EMAIL ADDRESS EXAMPLE\n",
    "\n",
    "[A-Za-z0-9\\._+]+@[A-Za-z]+\\.(com|org|edu|net)\n",
    "\n",
    "first part contains at least 1 upper/lower case letters, numbers 0-9, periods(.), plus signs (+), or underscores (_)\n",
    "next contains @\n",
    "then contians at least 1 upper/lower case letter\n",
    "followed by period (.)\n",
    "followed by .com/edu/net etc\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Writing Web Crawlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web crawlers are called such because they crawl across the web. At their core is an element of recursion. They must retrieve page contents for a URL, examine that page for another URL, and retrieve that page, ad infinitum.\n",
    "\n",
    "With web crawlers, you must be extremely conscientious of how much bandwidth you are using and make every effort to determine whether there’s a way to make the target server’s load easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traversing a single Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you’ll begin a project that will become a Six Degrees of Wikipedia solution finder: You’ll be able to take the Eric Idle page and find the fewest number of link clicks that will take you to the Kevin Bacon page.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kevin_Bacon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#bodyContent\n",
      "/wiki/Main_Page\n",
      "/wiki/Wikipedia:Contents\n",
      "/wiki/Portal:Current_events\n",
      "/wiki/Special:Random\n",
      "/wiki/Wikipedia:About\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "/wiki/Help:Contents\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Wikipedia:Community_portal\n",
      "/wiki/Special:RecentChanges\n",
      "/wiki/Wikipedia:File_upload_wizard\n",
      "/wiki/Main_Page\n",
      "/wiki/Special:Search\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Kevin+Bacon\n",
      "/w/index.php?title=Special:UserLogin&returnto=Kevin+Bacon\n",
      "/w/index.php?title=Special:CreateAccount&returnto=Kevin+Bacon\n",
      "/w/index.php?title=Special:UserLogin&returnto=Kevin+Bacon\n",
      "/wiki/Help:Introduction\n",
      "/wiki/Special:MyContributions\n",
      "/wiki/Special:MyTalk\n",
      "#\n",
      "#Early_life_and_education\n",
      "#Acting_career\n",
      "#Early_work\n",
      "#1980s\n",
      "#1990s\n",
      "#2000s\n",
      "#2010s\n",
      "#Other_ventures\n",
      "#Six_Degrees_of_Kevin_Bacon\n",
      "#Personal_life\n",
      "#Accolades\n",
      "#Awards_and_nominations\n",
      "#Other_honors\n",
      "#See_also\n",
      "#References\n",
      "#External_links\n",
      "https://af.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ar.wikipedia.org/wiki/%D9%83%D9%8A%D9%81%D9%8A%D9%86_%D8%A8%D9%8A%D9%83%D9%86\n",
      "https://an.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ast.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://azb.wikipedia.org/wiki/%DA%A9%D9%88%DB%8C%D9%86_%D8%A8%DB%8C%DA%A9%D9%86\n",
      "https://zh-min-nan.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://be.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D1%96%D0%BD_%D0%91%D1%8D%D0%B9%D0%BA%D0%B0%D0%BD\n",
      "https://bi.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://bg.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D0%B8%D0%BD_%D0%91%D0%B5%D0%B9%D0%BA%D1%8A%D0%BD\n",
      "https://bs.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ca.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://cs.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://da.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://de.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://el.wikipedia.org/wiki/%CE%9A%CE%AD%CE%B2%CE%B9%CE%BD_%CE%9C%CF%80%CE%AD%CE%B9%CE%BA%CE%BF%CE%BD\n",
      "https://eml.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://es.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://eo.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://eu.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://fa.wikipedia.org/wiki/%DA%A9%D9%88%DB%8C%D9%86_%D8%A8%DB%8C%DA%A9%D9%86\n",
      "https://fr.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ga.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://gl.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ko.wikipedia.org/wiki/%EC%BC%80%EB%B9%88_%EB%B2%A0%EC%9D%B4%EC%BB%A8\n",
      "https://hy.wikipedia.org/wiki/%D5%94%D6%87%D5%AB%D5%B6_%D4%B2%D5%A5%D5%B5%D6%84%D5%B8%D5%B6\n",
      "https://hr.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://io.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://id.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://it.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://he.wikipedia.org/wiki/%D7%A7%D7%95%D7%95%D7%99%D7%9F_%D7%91%D7%99%D7%99%D7%A7%D7%95%D7%9F\n",
      "https://ka.wikipedia.org/wiki/%E1%83%99%E1%83%94%E1%83%95%E1%83%98%E1%83%9C_%E1%83%91%E1%83%94%E1%83%98%E1%83%99%E1%83%9D%E1%83%9C%E1%83%98\n",
      "https://kk.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D0%B8%D0%BD_%D0%91%D1%8D%D0%B9%D0%BA%D0%BE%D0%BD\n",
      "https://lv.wikipedia.org/wiki/Kevins_Beikons\n",
      "https://hu.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://xmf.wikipedia.org/wiki/%E1%83%99%E1%83%94%E1%83%95%E1%83%98%E1%83%9C_%E1%83%91%E1%83%94%E1%83%98%E1%83%99%E1%83%9D%E1%83%9C%E1%83%98\n",
      "https://arz.wikipedia.org/wiki/%D9%83%D9%8A%D9%81%D9%8A%D9%86_%D8%A8%D9%8A%D9%83%D9%86\n",
      "https://ms.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://mn.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D0%B8%D0%BD_%D0%91%D1%8D%D0%B9%D0%BA%D0%BE%D0%BD\n",
      "https://nl.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ja.wikipedia.org/wiki/%E3%82%B1%E3%83%B4%E3%82%A3%E3%83%B3%E3%83%BB%E3%83%99%E3%83%BC%E3%82%B3%E3%83%B3\n",
      "https://no.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://oc.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://uz.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://pl.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://pt.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ro.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ru.wikipedia.org/wiki/%D0%91%D0%B5%D0%B9%D0%BA%D0%BE%D0%BD,_%D0%9A%D0%B5%D0%B2%D0%B8%D0%BD\n",
      "https://sco.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://simple.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://sk.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://ckb.wikipedia.org/wiki/%DA%A9%DB%8E%DA%A4%D9%86_%D8%A8%DB%95%DB%8C%DA%A9%D9%86\n",
      "https://sr.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D0%B8%D0%BD_%D0%91%D0%B5%D1%98%D0%BA%D0%BE%D0%BD\n",
      "https://sh.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://fi.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://sv.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%84%E0%B8%A7%E0%B8%B4%E0%B8%99_%E0%B9%80%E0%B8%9A%E0%B8%84%E0%B8%AD%E0%B8%99\n",
      "https://tr.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://uk.wikipedia.org/wiki/%D0%9A%D0%B5%D0%B2%D1%96%D0%BD_%D0%91%D0%B5%D0%B9%D0%BA%D0%BE%D0%BD\n",
      "https://vi.wikipedia.org/wiki/Kevin_Bacon\n",
      "https://wuu.wikipedia.org/wiki/%E5%87%AF%E6%96%87%C2%B7%E8%B4%9D%E8%82%AF\n",
      "https://zh-yue.wikipedia.org/wiki/%E5%A5%87%E9%9B%B2%E8%B2%9D%E6%A0%B9\n",
      "https://zh.wikipedia.org/wiki/%E5%87%AF%E6%96%87%C2%B7%E8%B4%9D%E8%82%AF\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q3454165#sitelinks-wikipedia\n",
      "/wiki/Kevin_Bacon\n",
      "/wiki/Talk:Kevin_Bacon\n",
      "/wiki/Kevin_Bacon\n",
      "/w/index.php?title=Kevin_Bacon&action=edit\n",
      "/w/index.php?title=Kevin_Bacon&action=history\n",
      "/wiki/Kevin_Bacon\n",
      "/w/index.php?title=Kevin_Bacon&action=edit\n",
      "/w/index.php?title=Kevin_Bacon&action=history\n",
      "/wiki/Special:WhatLinksHere/Kevin_Bacon\n",
      "/wiki/Special:RecentChangesLinked/Kevin_Bacon\n",
      "/wiki/Wikipedia:File_Upload_Wizard\n",
      "/wiki/Special:SpecialPages\n",
      "/w/index.php?title=Kevin_Bacon&oldid=1212708853\n",
      "/w/index.php?title=Kevin_Bacon&action=info\n",
      "/w/index.php?title=Special:CiteThisPage&page=Kevin_Bacon&id=1212708853&wpFormIdentifier=titleform\n",
      "/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKevin_Bacon\n",
      "/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKevin_Bacon\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q3454165\n",
      "/w/index.php?title=Special:DownloadAsPdf&page=Kevin_Bacon&action=show-download-screen\n",
      "/w/index.php?title=Kevin_Bacon&printable=yes\n",
      "https://commons.wikimedia.org/wiki/Kevin_Bacon\n",
      "/wiki/Wikipedia:Protection_policy#semi\n",
      "/wiki/Kevin_Bacon_(disambiguation)\n",
      "/wiki/File:Kevin_Bacon_SDCC_2014.jpg\n",
      "/wiki/Philadelphia\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "#cite_note-1\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "http://baconbros.com\n",
      "#cite_note-2\n",
      "#cite_note-actor-3\n",
      "/wiki/Leading_man\n",
      "/wiki/Character_actor\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/National_Lampoon%27s_Animal_House\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/The_River_Wild\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Crazy,_Stupid,_Love\n",
      "/wiki/X-Men:_First_Class\n",
      "/wiki/Patriots_Day_(film)\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/HBO\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Fox_Broadcasting_Company\n",
      "/wiki/The_Following\n",
      "/wiki/Amazon_Prime_Video\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/Showtime_(TV_network)\n",
      "/wiki/City_on_a_Hill_(TV_series)\n",
      "#cite_note-4\n",
      "/wiki/The_Guardian\n",
      "/wiki/Academy_Award\n",
      "#cite_note-5\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "#cite_note-6\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/EE_Limited\n",
      "#cite_note-7\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Philadelphia\n",
      "#cite_note-actor-3\n",
      "#cite_note-actor-3\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Urban_planning\n",
      "/wiki/Design_of_Cities\n",
      "#cite_note-bacon-8\n",
      "/wiki/Julia_R._Masterman_School\n",
      "/wiki/Spring_Garden,_Philadelphia\n",
      "#cite_note-9\n",
      "/wiki/Pennsylvania_Governor%27s_School_for_the_Arts\n",
      "/wiki/Bucknell_University\n",
      "/wiki/Lewisburg,_Pennsylvania\n",
      "#cite_note-10\n",
      "/wiki/Glory_Van_Scott\n",
      "#cite_note-walk-11\n",
      "#cite_note-bacon-8\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Circle_in_the_Square\n",
      "/wiki/Nancy_Mills\n",
      "/wiki/Cosmopolitan_(magazine)\n",
      "#cite_note-cosmo91-12\n",
      "/wiki/Fraternities_and_sororities\n",
      "/wiki/Animal_House\n",
      "#cite_note-bacon-8\n",
      "/wiki/Search_for_Tomorrow\n",
      "/wiki/Guiding_Light\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "#cite_note-13\n",
      "/wiki/Getting_Out\n",
      "/wiki/Phoenix_Theater\n",
      "/wiki/Flux\n",
      "/wiki/Second_Stage_Theatre\n",
      "#cite_note-bio-14\n",
      "/wiki/Obie_Award\n",
      "/wiki/Forty_Deuce\n",
      "#cite_note-kevin-15\n",
      "/wiki/Slab_Boys\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Val_Kilmer\n",
      "/wiki/Barry_Levinson\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/Steve_Guttenberg\n",
      "/wiki/Daniel_Stern_(actor)\n",
      "/wiki/Mickey_Rourke\n",
      "/wiki/Tim_Daly\n",
      "/wiki/Ellen_Barkin\n",
      "#cite_note-16\n",
      "/wiki/Footloose_(1984_film)\n",
      "#cite_note-bio-14\n",
      "/wiki/James_Dean\n",
      "/wiki/Rebel_Without_a_Cause\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Judy_Garland\n",
      "#cite_note-time84-17\n",
      "#cite_note-bacon-8\n",
      "#cite_note-18\n",
      "#cite_note-19\n",
      "/wiki/Typecasting_(acting)\n",
      "/wiki/John_Hughes_(filmmaker)\n",
      "/wiki/Planes,_Trains_and_Automobiles\n",
      "#cite_note-20\n",
      "/wiki/She%27s_Having_a_Baby\n",
      "#cite_note-bio-14\n",
      "/wiki/The_Big_Picture_(1989_film)\n",
      "#cite_note-21\n",
      "/wiki/Tremors_(1990_film)\n",
      "#cite_note-22\n",
      "/wiki/Joel_Schumacher\n",
      "/wiki/Flatliners\n",
      "#cite_note-bio-14\n",
      "/wiki/Elizabeth_Perkins\n",
      "/wiki/He_Said,_She_Said_(film)\n",
      "#cite_note-bio-14\n",
      "/wiki/The_New_York_Times\n",
      "#cite_note-nyt94-23\n",
      "/wiki/Oliver_Stone\n",
      "/wiki/JFK_(film)\n",
      "#cite_note-24\n",
      "/wiki/A_Few_Good_Men_(film)\n",
      "#cite_note-25\n",
      "/wiki/Michael_Greif\n",
      "#cite_note-bio-14\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/The_River_Wild\n",
      "#cite_note-bio-14\n",
      "/wiki/Meryl_Streep\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "#cite_note-bio-14\n",
      "/wiki/Blockbuster_(entertainment)\n",
      "/wiki/Apollo_13_(film)\n",
      "#cite_note-26\n",
      "/wiki/Sleepers_(film)\n",
      "#cite_note-27\n",
      "/wiki/Picture_Perfect_(1997_film)\n",
      "#cite_note-bio-14\n",
      "/wiki/Losing_Chase\n",
      "#cite_note-austin-28\n",
      "/wiki/Digging_to_China\n",
      "#cite_note-bio-14\n",
      "/wiki/Payola\n",
      "/wiki/Telling_Lies_in_America_(film)\n",
      "#cite_note-bio-14\n",
      "/wiki/Wild_Things_(film)\n",
      "/wiki/Stir_of_Echoes\n",
      "/wiki/David_Koepp\n",
      "#cite_note-29\n",
      "/wiki/File:Kevin_Bacon_Cannes_2004.jpg\n",
      "/wiki/Cannes_Film_Festival\n",
      "/wiki/Paul_Verhoeven\n",
      "/wiki/Hollow_Man\n",
      "#cite_note-30\n",
      "/wiki/Colin_Firth\n",
      "/wiki/Rachel_Blanchard\n",
      "/wiki/M%C3%A9nage_%C3%A0_trois\n",
      "/wiki/Where_the_Truth_Lies\n",
      "#cite_note-31\n",
      "/wiki/Atom_Egoyan\n",
      "/wiki/MPAA\n",
      "/wiki/MPAA_film_rating_system\n",
      "#cite_note-32\n",
      "/wiki/My_Dog_Skip_(film)\n",
      "#cite_note-33\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Tim_Robbins\n",
      "/wiki/Clint_Eastwood\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "#cite_note-34\n",
      "/wiki/Beauty_Shop\n",
      "/wiki/Queen_Latifah\n",
      "/wiki/HBO_Films\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/Desert_Storm\n",
      "#cite_note-35\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/File:Black_Mass_PC_04_(21429102646).jpg\n",
      "/wiki/Toronto_Film_Festival\n",
      "/wiki/Matthew_Vaughn\n",
      "/wiki/X-Men:_First_Class\n",
      "/wiki/Sebastian_Shaw_(comics)\n",
      "#cite_note-36\n",
      "#cite_note-37\n",
      "/wiki/Crazy,_Stupid,_Love\n",
      "/wiki/Dustin_Lance_Black\n",
      "/wiki/8_(play)\n",
      "/wiki/Perry_v._Brown\n",
      "/wiki/Proposition_8\n",
      "/wiki/Charles_J._Cooper\n",
      "#cite_note-8_the_play-38\n",
      "/wiki/Wilshire_Ebell_Theatre\n",
      "/wiki/American_Foundation_for_Equal_Rights\n",
      "#cite_note-8_play_video-39\n",
      "#cite_note-40\n",
      "/wiki/The_Following\n",
      "#cite_note-41\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "#cite_note-42\n",
      "/wiki/Black_Mass_(film)\n",
      "/wiki/Johnny_Depp\n",
      "/wiki/Huffington_Post\n",
      "/wiki/Tremors_(1990_film)\n",
      "#cite_note-43\n",
      "/wiki/Tremors_5:_Bloodline\n",
      "/wiki/Patriots_Day_(film)\n",
      "/wiki/Boston_Marathon_bombing\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "#cite_note-44\n",
      "/wiki/Instagram\n",
      "#cite_note-45\n",
      "/wiki/Old_97%27s\n",
      "/wiki/The_Guardians_of_the_Galaxy_Holiday_Special\n",
      "#cite_note-46\n",
      "/wiki/EE_(telecommunications_company)\n",
      "#cite_note-47\n",
      "#cite_note-48\n",
      "#cite_note-49\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Trivia\n",
      "/wiki/Big_screen\n",
      "/wiki/Six_degrees_of_separation\n",
      "/wiki/Internet_meme\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Social_networking_service\n",
      "#cite_note-50\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/IMDb\n",
      "#cite_note-51\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "#cite_note-52\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/PBS\n",
      "/wiki/Lanford_Wilson\n",
      "/wiki/Lemon_Sky\n",
      "#cite_note-cosmo91-12\n",
      "/wiki/Pyrates\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/w/index.php?title=Travis_Sedgwick&action=edit&redlink=1\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Upper_West_Side\n",
      "/wiki/Manhattan\n",
      "#cite_note-53\n",
      "/wiki/Tracy_Pollan\n",
      "#cite_note-54\n",
      "/wiki/Separation_of_church_and_state_in_the_United_States\n",
      "#cite_note-55\n",
      "#cite_note-56\n",
      "/wiki/The_Times\n",
      "/wiki/Atheism\n",
      "#cite_note-57\n",
      "/wiki/Antireligion\n",
      "#cite_note-58\n",
      "/wiki/Will.i.am\n",
      "/wiki/It%27s_a_New_Day_(Will.i.am_song)\n",
      "/wiki/Barack_Obama\n",
      "/wiki/Ponzi_scheme\n",
      "/wiki/Bernie_Madoff\n",
      "#cite_note-financialpost-59\n",
      "#cite_note-60\n",
      "/wiki/Finding_Your_Roots\n",
      "/wiki/Henry_Louis_Gates\n",
      "#cite_note-61\n",
      "#cite_note-62\n",
      "#cite_note-63\n",
      "#cite_note-64\n",
      "/wiki/Apollo_13_(film)\n",
      "#cite_note-65\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "#cite_note-66\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Boston_Society_of_Film_Critics\n",
      "/wiki/Boston_Society_of_Film_Critics_Award_for_Best_Cast\n",
      "#cite_note-67\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/CableACE_Award\n",
      "/wiki/CableACE_Award\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Chlotrudis_Awards\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Critics%27_Choice_Movie_Awards\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Digging_to_China\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Golden_Globe_Award_for_Best_Supporting_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_River_Wild\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Independent_Spirit_Awards\n",
      "/wiki/Independent_Spirit_Award_for_Best_Male_Lead\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/MTV_Movie_%26_TV_Awards\n",
      "/wiki/MTV_Movie_Award_for_Best_Villain\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Taking_Chance\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/Primetime_Emmy_Award_for_Outstanding_Lead_Actor_in_a_Limited_Series_or_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Satellite_Awards\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Saturn_Award\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Scream_Awards\n",
      "/wiki/Scream_Awards\n",
      "/wiki/X-Men:_First_Class\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Supporting_Role\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Teen_Choice_Awards\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/Beauty_Shop\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/X-Men:_First_Class\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/The_Following\n",
      "/wiki/File:Kevin_Bacon%27s_Star_Walk_of_Fame.jpg\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "#cite_note-68\n",
      "/wiki/Denver_Film_Festival\n",
      "#cite_note-69\n",
      "/wiki/Phoenix_Film_Festival\n",
      "#cite_note-70\n",
      "/wiki/Santa_Barbara_International_Film_Festival\n",
      "#cite_note-71\n",
      "/wiki/Broadcast_Film_Critics_Association\n",
      "#cite_note-72\n",
      "/wiki/Seattle_International_Film_Festival\n",
      "#cite_note-73\n",
      "/wiki/List_of_actors_with_Hollywood_Walk_of_Fame_motion_picture_stars\n",
      "#cite_ref-1\n",
      "http://www.geneall.net/U/per_page.php?id=1014399\n",
      "#cite_ref-2\n",
      "https://web.archive.org/web/20090113222205/http://www.newenglandancestors.org/research/services/articles_gbr78.asp\n",
      "http://www.newenglandancestors.org/research/services/articles_gbr78.asp\n",
      "#cite_ref-actor_3-0\n",
      "#cite_ref-actor_3-1\n",
      "#cite_ref-actor_3-2\n",
      "http://www.biography.com/people/kevin-bacon-9542173\n",
      "#cite_ref-4\n",
      "https://www.hollywoodreporter.com/tv/tv-news/showtime-cancels-city-on-a-hill-3-seasons-1235250089/\n",
      "/wiki/The_Hollywood_Reporter\n",
      "#cite_ref-5\n",
      "https://www.theguardian.com/film/filmblog/2009/feb/19/best-actors-never-nominated-for-oscars\n",
      "#cite_ref-6\n",
      "http://www.walkoffame.com/kevin-bacon\n",
      "#cite_ref-7\n",
      "https://www.marketingweek.com/ee-unveils-six-degrees-of-bacon-launch-ads/\n",
      "#cite_ref-bacon_8-0\n",
      "#cite_ref-bacon_8-1\n",
      "#cite_ref-bacon_8-2\n",
      "#cite_ref-bacon_8-3\n",
      "https://web.archive.org/web/20190403203113/https://www.biography.com/news/kevin-bacon-biography-facts\n",
      "http://www.biography.com/news/kevin-bacon-biography-facts\n",
      "#cite_ref-9\n",
      "https://philadelphia.cbslocal.com/top-lists/stars-from-philly-to-hollywood/\n",
      "#cite_ref-10\n",
      "https://movies.yahoo.com/person/kevin-bacon/biography.html\n",
      "#cite_ref-walk_11-0\n",
      "https://web.archive.org/web/20141016202657/http://www.thebiographychannel.co.uk/biographies/kevin-bacon.html\n",
      "http://www.thebiographychannel.co.uk/biographies/kevin-bacon.html\n",
      "#cite_ref-cosmo91_12-0\n",
      "#cite_ref-cosmo91_12-1\n",
      "#cite_ref-13\n",
      "http://www.nydailynews.com/entertainment/happy-halloween-superstars-start-horror-flick-gallery-1.98345\n",
      "#cite_ref-bio_14-0\n",
      "#cite_ref-bio_14-1\n",
      "#cite_ref-bio_14-2\n",
      "#cite_ref-bio_14-3\n",
      "#cite_ref-bio_14-4\n",
      "#cite_ref-bio_14-5\n",
      "#cite_ref-bio_14-6\n",
      "#cite_ref-bio_14-7\n",
      "#cite_ref-bio_14-8\n",
      "#cite_ref-bio_14-9\n",
      "#cite_ref-bio_14-10\n",
      "https://web.archive.org/web/20160530033240/http://www.pbs.org/weta/finding-your-roots/profiles/kevin-bacon%c2%a0/\n",
      "https://www.pbs.org/wnet/finding-your-roots/profiles/kevin-bacon%C2%A0/\n",
      "#cite_ref-kevin_15-0\n",
      "https://www.tvguide.com/celebrities/kevin-bacon/bio/160550\n",
      "#cite_ref-16\n",
      "https://web.archive.org/web/20141021030336/http://news.moviefone.com/2012/03/02/diner-30th-anniversary/\n",
      "http://news.moviefone.com/2012/03/02/diner-30th-anniversary/\n",
      "#cite_ref-time84_17-0\n",
      "https://web.archive.org/web/20080830035710/http://www.time.com/time/magazine/article/0,9171,950019,00.html\n",
      "http://www.time.com/time/magazine/article/0,9171,950019,00.html\n",
      "#cite_ref-18\n",
      "http://www.huffingtonpost.com/2014/08/25/kevin-bacon-footloose_n_5710413.html\n",
      "#cite_ref-19\n",
      "https://web.archive.org/web/20090109152125/http://www.thebiographychannel.co.uk/biography_story/522%3A492/1/Kevin_Bacon.htm\n",
      "http://www.thebiographychannel.co.uk/biography_story/522:492/1/Kevin_Bacon.htm\n",
      "#cite_ref-20\n",
      "https://www.mentalfloss.com/article/71594/14-moving-facts-about-planes-trains-and-automobiles\n",
      "#cite_ref-21\n",
      "https://www.nytimes.com/1994/09/25/movies/a-second-wind-is-blowing-for-kevin-bacon.html\n",
      "#cite_ref-22\n",
      "https://www.nytimes.com/movie/review?res=9C0CE2DE1631F93AA25752C0A966958260\n",
      "#cite_ref-nyt94_23-0\n",
      "https://query.nytimes.com/gst/fullpage.html?res=9C07E6D91F3BF936A1575AC0A962958260\n",
      "#cite_ref-24\n",
      "http://www.jfk-online.com/jfkbacon.html\n",
      "#cite_ref-25\n",
      "http://www.tcm.com/this-month/article/143158%7C0/A-Few-Good-Men.html\n",
      "#cite_ref-26\n",
      "http://collider.com/kevin-bacon-commercials-footloose/\n",
      "#cite_ref-27\n",
      "http://www.rogerebert.com/reviews/sleepers-1996\n",
      "#cite_ref-austin_28-0\n",
      "http://www.austinchronicle.com/calendar/film/1997-02-07/283342/\n",
      "/wiki/The_Austin_Chronicle\n",
      "#cite_ref-29\n",
      "http://www.criminalelement.com/blogs/2013/09/under-the-raderhorror-movies-you-may-have-missed-stir-of-echoes\n",
      "#cite_ref-30\n",
      "http://www.rogerebert.com/reviews/hollow-man-2000\n",
      "#cite_ref-31\n",
      "https://web.archive.org/web/20141017080013/http://movies.about.com/od/wherethetruthlies/a/truthkb101305.htm\n",
      "http://movies.about.com/od/wherethetruthlies/a/truthkb101305.htm\n",
      "#cite_ref-32\n",
      "https://archive.today/20120604150801/http://jam.canoe.ca/Movies/2005/09/14/1216527.html\n",
      "/wiki/Template:Cite_news\n",
      "/wiki/Category:CS1_maint:_unfit_URL\n",
      "#cite_ref-33\n",
      "https://archive.nytimes.com/www.nytimes.com/library/film/011200skip-film-review.html\n",
      "#cite_ref-34\n",
      "https://web.archive.org/web/20140802203026/http://www.latimes.com/entertainment/la-et-kevin-bacon-photo6-photo.html\n",
      "http://www.latimes.com/entertainment/la-et-kevin-bacon-photo6-photo.html\n",
      "#cite_ref-35\n",
      "http://www.nydailynews.com/entertainment/tv-movies/kevin-bacon-chance-body-fallen-marine-home-article-1.392226\n",
      "#cite_ref-36\n",
      "https://web.archive.org/web/20100722010545/http://heatvision.hollywoodreporter.com/2010/07/winters-bone-star-cast-as-mystique-in-xmen-first-class.html\n",
      "http://heatvision.hollywoodreporter.com/2010/07/winters-bone-star-cast-as-mystique-in-xmen-first-class.html\n",
      "#cite_ref-37\n",
      "https://web.archive.org/web/20100720060214/http://www.forcesofgeek.com/2010/07/kevin-bacon-playing-sebastian-shaw-in-x.html\n",
      "http://www.forcesofgeek.com/2010/07/kevin-bacon-playing-sebastian-shaw-in-x.html\n",
      "#cite_ref-8_the_play_38-0\n",
      "http://www.accesshollywood.com/jesse-tyler-ferguson/glee-stars-touched-by-brad-pitt-and-george-clooneys-support-of-8_article_61543\n",
      "/wiki/Access_Hollywood\n",
      "#cite_ref-8_play_video_39-0\n",
      "https://www.youtube.com/watch?v=qlUG8F9uVgM\n",
      "https://ghostarchive.org/varchive/youtube/20211211/qlUG8F9uVgM\n",
      "#cite_ref-40\n",
      "http://www.pinknews.co.uk/2012/03/01/youtube-to-broadcast-proposition-8-play-live/\n",
      "#cite_ref-41\n",
      "http://www.fox.com/the-following/\n",
      "#cite_ref-42\n",
      "https://news.yahoo.com/blogs/trending-now/kevin-bacon-gives-millennials-a-history-lesson-about-the--80s-162525915.html\n",
      "#cite_ref-43\n",
      "http://www.huffingtonpost.com.au/entry/kevin-bacon-tremors-tv-reboot_us_5655b651e4b072e9d1c13a11\n",
      "#cite_ref-44\n",
      "http://baconbros.com/\n",
      "#cite_ref-45\n",
      "https://www.instagram.com/p/ClUvj92p4Qn/?hl=en\n",
      "#cite_ref-46\n",
      "https://www.marvel.com/articles/tv-shows/guardians-of-the-galaxy-holiday-special-soundtrack\n",
      "#cite_ref-47\n",
      "http://www.campaignlive.co.uk/news/1294856/\n",
      "#cite_ref-48\n",
      "http://parade.condenast.com/269380/ashleighschmitz/kevin-bacon-reprises-his-most-iconic-film-roles-in-british-commercial/\n",
      "#cite_ref-49\n",
      "https://money.cnn.com/2015/03/13/media/kevin-bacon-eggs/index.html\n",
      "/wiki/CNN\n",
      "#cite_ref-50\n",
      "http://www.sixdegrees.org/\n",
      "#cite_ref-51\n",
      "http://www.webmonkey.com/2012/09/easter-egg-google-connects-the-dots-for-bacon-number-search/\n",
      "#cite_ref-52\n",
      "https://web.archive.org/web/20121112081753/http://www.telegraph.co.uk/science/science-news/4768389/And-the-winner-tonight-is.html\n",
      "https://www.telegraph.co.uk/science/science-news/4768389/And-the-winner-tonight-is.html\n",
      "#cite_ref-53\n",
      "http://www.nydailynews.com/entertainment/tv-movies/kevin-bacon-loyalty-nyc-philly-origins-peace-bustling-city-article-1.147197\n",
      "#cite_ref-54\n",
      "http://www.people.com/people/archive/article/0,,20093025,00.html\n",
      "#cite_ref-55\n",
      "https://web.archive.org/web/20141023014658/https://www.au.org/media/church-and-state/archives/2008/05/two-thumbs-up.html\n",
      "http://www.au.org/media/church-and-state/archives/2008/05/two-thumbs-up.html\n",
      "#cite_ref-56\n",
      "https://www.washingtonpost.com/wp-dyn/content/article/2008/03/25/AR2008032503852.html\n",
      "#cite_ref-57\n",
      "#cite_ref-58\n",
      "http://www.foxnews.com/story/0,2933,343589,00.html\n",
      "#cite_ref-financialpost_59-0\n",
      "https://web.archive.org/web/20140314085857/http://economiccrisis.us/2009/06/may-god-spare-mercy-victim-tells-madoff/\n",
      "http://economiccrisis.us/2009/06/may-god-spare-mercy-victim-tells-madoff/\n",
      "#cite_ref-60\n",
      "#cite_ref-61\n",
      "http://www.huffingtonpost.com/megan-smolenyak-smolenyak/6-degrees-of-separation-k_b_900707.html\n",
      "#cite_ref-62\n",
      "https://web.archive.org/web/20130405182304/http://www.drawtheline.org/watch-stuff/\n",
      "http://www.drawtheline.org/watch-stuff\n",
      "#cite_ref-63\n",
      "http://www.drawtheline.org/sign-now/\n",
      "#cite_ref-64\n",
      "https://web.archive.org/web/20190214002809/http://www.awardscircuit.com/2015/07/01/pixars-toy-story-wins-top-prize-for-1995-awards-circuit-community-awards/\n",
      "http://www.awardscircuit.com/2015/07/01/pixars-toy-story-wins-top-prize-for-1995-awards-circuit-community-awards/\n",
      "#cite_ref-65\n",
      "https://web.archive.org/web/20190520150327/http://www.awardscircuit.com/acca/the-acca-nominations-2003/\n",
      "http://www.awardscircuit.com/acca/the-acca-nominations-2003/\n",
      "#cite_ref-66\n",
      "https://idobi.com/news/nsync-takes-home-three-blockbuster-entertainment-awards/\n",
      "#cite_ref-67\n",
      "https://www.upi.com/Boston-critics-pick-Mystic-River/24961071506935/\n",
      "#cite_ref-68\n",
      "http://www.walkoffame.com/kevin-bacon\n",
      "#cite_ref-69\n",
      "https://www.imdb.com/event/ev0000209/2004/1/\n",
      "#cite_ref-70\n",
      "https://www.imdb.com/event/ev0000536/2005/1/\n",
      "#cite_ref-71\n",
      "https://www.imdb.com/event/ev0000589/2005/1/\n",
      "#cite_ref-72\n",
      "https://archive.today/20180123031007/http://www.criticschoice.com/movie-awards/critics%E2%80%99-choice-movie-awards-winners-archive/\n",
      "http://www.criticschoice.com/movie-awards/critics%E2%80%99-choice-movie-awards-winners-archive/\n",
      "#cite_ref-73\n",
      "https://www.imdb.com/event/ev0000600/2015/1/\n",
      "https://commons.wikimedia.org/wiki/Category:Kevin_Bacon\n",
      "https://www.imdb.com/name/nm0000102/\n",
      "/wiki/IMDb_(identifier)\n",
      "https://www.ibdb.com/broadway-cast-staff/90569\n",
      "/wiki/Internet_Broadway_Database\n",
      "https://www.wikidata.org/wiki/Q3454165#P1220\n",
      "http://www.iobdb.com/CreditableEntity/5597\n",
      "/wiki/Internet_Off-Broadway_Database\n",
      "https://www.allmovie.com/artist/p3164\n",
      "/wiki/AllMovie\n",
      "http://oracleofbacon.org\n",
      "/wiki/Template:Kevin_Bacon\n",
      "/wiki/Template_talk:Kevin_Bacon\n",
      "/wiki/Special:EditPage/Template:Kevin_Bacon\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Template:Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Template_talk:Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Special:EditPage/Template:Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Ian_McKellen\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jamie_Foxx\n",
      "/wiki/Philip_Seymour_Hoffman\n",
      "/wiki/Forest_Whitaker\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jeff_Bridges\n",
      "/wiki/Colin_Firth\n",
      "/wiki/George_Clooney\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Matthew_McConaughey\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Leonardo_DiCaprio\n",
      "/wiki/Casey_Affleck\n",
      "/wiki/Gary_Oldman\n",
      "/wiki/Christian_Bale\n",
      "/wiki/Joaquin_Phoenix\n",
      "/wiki/Chadwick_Boseman\n",
      "/wiki/Will_Smith\n",
      "/wiki/Brendan_Fraser\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Template:Golden_Globe_Best_Actor_TV_Miniseries_Film\n",
      "/wiki/Template_talk:Golden_Globe_Best_Actor_TV_Miniseries_Film\n",
      "/wiki/Special:EditPage/Template:Golden_Globe_Best_Actor_TV_Miniseries_Film\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Anthony_Andrews\n",
      "/wiki/Richard_Chamberlain\n",
      "/wiki/Ted_Danson\n",
      "/wiki/Dustin_Hoffman\n",
      "/wiki/James_Woods\n",
      "/wiki/Randy_Quaid\n",
      "/wiki/Michael_Caine\n",
      "/wiki/Stacy_Keach\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Beau_Bridges\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Ving_Rhames\n",
      "/wiki/Stanley_Tucci\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/James_Franco\n",
      "/wiki/Albert_Finney\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jonathan_Rhys_Meyers\n",
      "/wiki/Bill_Nighy\n",
      "/wiki/Jim_Broadbent\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Billy_Bob_Thornton\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Tom_Hiddleston\n",
      "/wiki/Ewan_McGregor\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Evan_Peters\n",
      "/wiki/Steven_Yeun\n",
      "/wiki/Template:Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Template_talk:Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Special:EditPage/Template:Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Kyle_Chandler\n",
      "/wiki/Steven_Weber_(actor)\n",
      "/wiki/Richard_Dean_Anderson\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Robert_Patrick\n",
      "/wiki/Ben_Browder\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Ben_Browder\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Michael_C._Hall\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Edward_James_Olmos\n",
      "/wiki/Josh_Holloway\n",
      "/wiki/Stephen_Moyer\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Mads_Mikkelsen\n",
      "/wiki/Hugh_Dancy\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Bruce_Campbell\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Kyle_MacLachlan\n",
      "/wiki/Patrick_Stewart\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Network_or_Cable_Television_Series\n",
      "/wiki/Sam_Heughan\n",
      "/wiki/Bob_Odenkirk\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Streaming_Television_Series\n",
      "/wiki/Henry_Thomas\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Patrick_Stewart\n",
      "/wiki/Template:ScreenActorsGuildAward_MaleTVMiniseriesMovie\n",
      "/wiki/Template_talk:ScreenActorsGuildAward_MaleTVMiniseriesMovie\n",
      "/wiki/Special:EditPage/Template:ScreenActorsGuildAward_MaleTVMiniseriesMovie\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Christopher_Reeve\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/Ben_Kingsley\n",
      "/wiki/William_H._Macy\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Paul_Newman\n",
      "/wiki/Jeremy_Irons\n",
      "/wiki/Kevin_Kline\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Alexander_Skarsg%C3%A5rd\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Sam_Rockwell\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Sam_Elliott\n",
      "/wiki/Steven_Yeun\n",
      "/wiki/Help:Authority_control\n",
      "https://www.wikidata.org/wiki/Q3454165#identifiers\n",
      "http://id.worldcat.org/fast/242183/\n",
      "https://isni.org/isni/0000000121291300\n",
      "https://viaf.org/viaf/39570812\n",
      "https://authority.bibsys.no/authority/rest/authorities/html/98015093\n",
      "http://catalogo.bne.es/uhtbin/authoritybrowse.cgi?action=display&authority_id=XX1298810\n",
      "https://catalogue.bnf.fr/ark:/12148/cb139817766\n",
      "https://data.bnf.fr/ark:/12148/cb139817766\n",
      "https://d-nb.info/gnd/124109659\n",
      "http://olduli.nli.org.il/F/?func=find-b&local_base=NLX10&find_code=UID&request=987007454685905171\n",
      "https://id.loc.gov/authorities/n88034930\n",
      "https://kopkatalogs.lv/F?func=direct&local_base=lnc10&doc_number=000249798&P_CON_LNG=ENG\n",
      "https://aleph.nkp.cz/F/?func=find-c&local_base=aut&ccl_term=ica=xx0025279&CON_LNG=ENG\n",
      "https://nla.gov.au/anbd.aut-an36021861\n",
      "https://lod.nl.go.kr/resource/KAC2020K8226\n",
      "http://data.bibliotheken.nl/id/thes/p147880998\n",
      "https://dbn.bn.org.pl/descriptor-details/9810630778405606\n",
      "https://musicbrainz.org/artist/cc0dbdfc-9b2c-4e31-8448-808412388406\n",
      "https://trove.nla.gov.au/people/1189569\n",
      "https://snaccooperative.org/ark:/99166/w6w67gw2\n",
      "https://www.idref.fr/067287832\n",
      "https://en.wikipedia.org/w/index.php?title=Kevin_Bacon&oldid=1212708853\n",
      "/wiki/Help:Category\n",
      "/wiki/Category:1958_births\n",
      "/wiki/Category:20th-century_American_male_actors\n",
      "/wiki/Category:21st-century_American_male_actors\n",
      "/wiki/Category:American_atheists\n",
      "/wiki/Category:American_male_film_actors\n",
      "/wiki/Category:American_male_soap_opera_actors\n",
      "/wiki/Category:American_male_television_actors\n",
      "/wiki/Category:American_male_voice_actors\n",
      "/wiki/Category:The_Bacon_Brothers_members\n",
      "/wiki/Category:Best_Miniseries_or_Television_Movie_Actor_Golden_Globe_winners\n",
      "/wiki/Category:Circle_in_the_Square_Theatre_School_alumni\n",
      "/wiki/Category:Living_people\n",
      "/wiki/Category:Male_actors_from_Philadelphia\n",
      "/wiki/Category:Obie_Award_recipients\n",
      "/wiki/Category:Outstanding_Performance_by_a_Cast_in_a_Motion_Picture_Screen_Actors_Guild_Award_winners\n",
      "/wiki/Category:Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie_Screen_Actors_Guild_Award_winners\n",
      "/wiki/Category:Sedgwick_family\n",
      "/wiki/Category:Male_actors_from_Manhattan\n",
      "/wiki/Category:People_from_the_Upper_West_Side\n",
      "/wiki/Category:CS1_maint:_unfit_URL\n",
      "/wiki/Category:Use_American_English_from_April_2023\n",
      "/wiki/Category:All_Wikipedia_articles_written_in_American_English\n",
      "/wiki/Category:Articles_with_short_description\n",
      "/wiki/Category:Short_description_matches_Wikidata\n",
      "/wiki/Category:Wikipedia_indefinitely_semi-protected_biographies_of_living_people\n",
      "/wiki/Category:Use_mdy_dates_from_April_2023\n",
      "/wiki/Category:Articles_with_hCards\n",
      "/wiki/Category:Commons_category_link_from_Wikidata\n",
      "/wiki/Category:IBDB_name_template_using_Wikidata\n",
      "/wiki/Category:Internet_Off-Broadway_Database_person_ID_same_as_Wikidata\n",
      "/wiki/Category:Articles_with_FAST_identifiers\n",
      "/wiki/Category:Articles_with_ISNI_identifiers\n",
      "/wiki/Category:Articles_with_VIAF_identifiers\n",
      "/wiki/Category:Articles_with_BIBSYS_identifiers\n",
      "/wiki/Category:Articles_with_BNE_identifiers\n",
      "/wiki/Category:Articles_with_BNF_identifiers\n",
      "/wiki/Category:Articles_with_BNFdata_identifiers\n",
      "/wiki/Category:Articles_with_GND_identifiers\n",
      "/wiki/Category:Articles_with_J9U_identifiers\n",
      "/wiki/Category:Articles_with_LCCN_identifiers\n",
      "/wiki/Category:Articles_with_LNB_identifiers\n",
      "/wiki/Category:Articles_with_NKC_identifiers\n",
      "/wiki/Category:Articles_with_NLA_identifiers\n",
      "/wiki/Category:Articles_with_NLK_identifiers\n",
      "/wiki/Category:Articles_with_NTA_identifiers\n",
      "/wiki/Category:Articles_with_PLWABN_identifiers\n",
      "/wiki/Category:Articles_with_MusicBrainz_identifiers\n",
      "/wiki/Category:Articles_with_Trove_identifiers\n",
      "/wiki/Category:Articles_with_SNAC-ID_identifiers\n",
      "/wiki/Category:Articles_with_SUDOC_identifiers\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\n",
      "//en.wikipedia.org/wiki/Wikipedia:Text_of_the_Creative_Commons_Attribution-ShareAlike_4.0_International_License\n",
      "//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Terms_of_Use\n",
      "//foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\n",
      "//www.wikimediafoundation.org/\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Privacy_policy\n",
      "/wiki/Wikipedia:About\n",
      "/wiki/Wikipedia:General_disclaimer\n",
      "//en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Universal_Code_of_Conduct\n",
      "https://developer.wikimedia.org\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://foundation.wikimedia.org/wiki/Special:MyLanguage/Policy:Cookie_statement\n",
      "//en.m.wikipedia.org/w/index.php?title=Kevin_Bacon&mobileaction=toggle_view_mobile\n",
      "https://wikimediafoundation.org/\n",
      "https://www.mediawiki.org/\n"
     ]
    }
   ],
   "source": [
    "# You should already know how to write a Python script that retrieves an arbitrary\n",
    "# Wikipedia page and produces a list of links on that page:\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "for link in bs.find_all('a'):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently a friend of mine, while working on a similar Wikipedia-scraping project, mentioned he had written a large filtering function, with more than 100 lines of code, in order to determine whether an internal Wikipedia link was an article page.\n",
    "\n",
    "Unfortunately, he had not spent much time upfront trying to find patterns between “article links” and “other links,”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the links that point to article pages (as opposed to other internal pages), you’ll see that they all have three things in common:\n",
    "\n",
    "* They reside within the div with the id set to bodyContent.\n",
    "\n",
    "* The URLs do not contain colons.\n",
    "* The URLs begin with /wiki/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Kevin_Bacon_(disambiguation)\n",
      "/wiki/Philadelphia\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/Leading_man\n",
      "/wiki/Character_actor\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/National_Lampoon%27s_Animal_House\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/The_River_Wild\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Crazy,_Stupid,_Love\n",
      "/wiki/Patriots_Day_(film)\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/HBO\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Fox_Broadcasting_Company\n",
      "/wiki/The_Following\n",
      "/wiki/Amazon_Prime_Video\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/Showtime_(TV_network)\n",
      "/wiki/City_on_a_Hill_(TV_series)\n",
      "/wiki/The_Guardian\n",
      "/wiki/Academy_Award\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/EE_Limited\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Philadelphia\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Urban_planning\n",
      "/wiki/Design_of_Cities\n",
      "/wiki/Julia_R._Masterman_School\n",
      "/wiki/Spring_Garden,_Philadelphia\n",
      "/wiki/Pennsylvania_Governor%27s_School_for_the_Arts\n",
      "/wiki/Bucknell_University\n",
      "/wiki/Lewisburg,_Pennsylvania\n",
      "/wiki/Glory_Van_Scott\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Circle_in_the_Square\n",
      "/wiki/Nancy_Mills\n",
      "/wiki/Cosmopolitan_(magazine)\n",
      "/wiki/Fraternities_and_sororities\n",
      "/wiki/Animal_House\n",
      "/wiki/Search_for_Tomorrow\n",
      "/wiki/Guiding_Light\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "/wiki/Getting_Out\n",
      "/wiki/Phoenix_Theater\n",
      "/wiki/Flux\n",
      "/wiki/Second_Stage_Theatre\n",
      "/wiki/Obie_Award\n",
      "/wiki/Forty_Deuce\n",
      "/wiki/Slab_Boys\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Val_Kilmer\n",
      "/wiki/Barry_Levinson\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/Steve_Guttenberg\n",
      "/wiki/Daniel_Stern_(actor)\n",
      "/wiki/Mickey_Rourke\n",
      "/wiki/Tim_Daly\n",
      "/wiki/Ellen_Barkin\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/James_Dean\n",
      "/wiki/Rebel_Without_a_Cause\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Judy_Garland\n",
      "/wiki/Typecasting_(acting)\n",
      "/wiki/John_Hughes_(filmmaker)\n",
      "/wiki/Planes,_Trains_and_Automobiles\n",
      "/wiki/She%27s_Having_a_Baby\n",
      "/wiki/The_Big_Picture_(1989_film)\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/Joel_Schumacher\n",
      "/wiki/Flatliners\n",
      "/wiki/Elizabeth_Perkins\n",
      "/wiki/He_Said,_She_Said_(film)\n",
      "/wiki/The_New_York_Times\n",
      "/wiki/Oliver_Stone\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men_(film)\n",
      "/wiki/Michael_Greif\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/The_River_Wild\n",
      "/wiki/Meryl_Streep\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Blockbuster_(entertainment)\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Sleepers_(film)\n",
      "/wiki/Picture_Perfect_(1997_film)\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Digging_to_China\n",
      "/wiki/Payola\n",
      "/wiki/Telling_Lies_in_America_(film)\n",
      "/wiki/Wild_Things_(film)\n",
      "/wiki/Stir_of_Echoes\n",
      "/wiki/David_Koepp\n",
      "/wiki/Cannes_Film_Festival\n",
      "/wiki/Paul_Verhoeven\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Colin_Firth\n",
      "/wiki/Rachel_Blanchard\n",
      "/wiki/M%C3%A9nage_%C3%A0_trois\n",
      "/wiki/Where_the_Truth_Lies\n",
      "/wiki/Atom_Egoyan\n",
      "/wiki/MPAA\n",
      "/wiki/MPAA_film_rating_system\n",
      "/wiki/My_Dog_Skip_(film)\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Tim_Robbins\n",
      "/wiki/Clint_Eastwood\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Beauty_Shop\n",
      "/wiki/Queen_Latifah\n",
      "/wiki/HBO_Films\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/Desert_Storm\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Toronto_Film_Festival\n",
      "/wiki/Matthew_Vaughn\n",
      "/wiki/Sebastian_Shaw_(comics)\n",
      "/wiki/Crazy,_Stupid,_Love\n",
      "/wiki/Dustin_Lance_Black\n",
      "/wiki/8_(play)\n",
      "/wiki/Perry_v._Brown\n",
      "/wiki/Proposition_8\n",
      "/wiki/Charles_J._Cooper\n",
      "/wiki/Wilshire_Ebell_Theatre\n",
      "/wiki/American_Foundation_for_Equal_Rights\n",
      "/wiki/The_Following\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Black_Mass_(film)\n",
      "/wiki/Johnny_Depp\n",
      "/wiki/Huffington_Post\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/Patriots_Day_(film)\n",
      "/wiki/Boston_Marathon_bombing\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/Instagram\n",
      "/wiki/Old_97%27s\n",
      "/wiki/The_Guardians_of_the_Galaxy_Holiday_Special\n",
      "/wiki/EE_(telecommunications_company)\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Trivia\n",
      "/wiki/Big_screen\n",
      "/wiki/Six_degrees_of_separation\n",
      "/wiki/Internet_meme\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Social_networking_service\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/IMDb\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/PBS\n",
      "/wiki/Lanford_Wilson\n",
      "/wiki/Lemon_Sky\n",
      "/wiki/Pyrates\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Upper_West_Side\n",
      "/wiki/Manhattan\n",
      "/wiki/Tracy_Pollan\n",
      "/wiki/Separation_of_church_and_state_in_the_United_States\n",
      "/wiki/The_Times\n",
      "/wiki/Atheism\n",
      "/wiki/Antireligion\n",
      "/wiki/Will.i.am\n",
      "/wiki/It%27s_a_New_Day_(Will.i.am_song)\n",
      "/wiki/Barack_Obama\n",
      "/wiki/Ponzi_scheme\n",
      "/wiki/Bernie_Madoff\n",
      "/wiki/Finding_Your_Roots\n",
      "/wiki/Henry_Louis_Gates\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Boston_Society_of_Film_Critics\n",
      "/wiki/Boston_Society_of_Film_Critics_Award_for_Best_Cast\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/CableACE_Award\n",
      "/wiki/CableACE_Award\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Chlotrudis_Awards\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Critics%27_Choice_Movie_Awards\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Digging_to_China\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Golden_Globe_Award_for_Best_Supporting_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_River_Wild\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Independent_Spirit_Awards\n",
      "/wiki/Independent_Spirit_Award_for_Best_Male_Lead\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/MTV_Movie_%26_TV_Awards\n",
      "/wiki/MTV_Movie_Award_for_Best_Villain\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Taking_Chance\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/Primetime_Emmy_Award_for_Outstanding_Lead_Actor_in_a_Limited_Series_or_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Satellite_Awards\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Saturn_Award\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Scream_Awards\n",
      "/wiki/Scream_Awards\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Supporting_Role\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Teen_Choice_Awards\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/Beauty_Shop\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/The_Following\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Denver_Film_Festival\n",
      "/wiki/Phoenix_Film_Festival\n",
      "/wiki/Santa_Barbara_International_Film_Festival\n",
      "/wiki/Broadcast_Film_Critics_Association\n",
      "/wiki/Seattle_International_Film_Festival\n",
      "/wiki/List_of_actors_with_Hollywood_Walk_of_Fame_motion_picture_stars\n",
      "/wiki/The_Hollywood_Reporter\n",
      "/wiki/The_Austin_Chronicle\n",
      "/wiki/Access_Hollywood\n",
      "/wiki/CNN\n",
      "/wiki/IMDb_(identifier)\n",
      "/wiki/Internet_Broadway_Database\n",
      "/wiki/Internet_Off-Broadway_Database\n",
      "/wiki/AllMovie\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Ian_McKellen\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jamie_Foxx\n",
      "/wiki/Philip_Seymour_Hoffman\n",
      "/wiki/Forest_Whitaker\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jeff_Bridges\n",
      "/wiki/Colin_Firth\n",
      "/wiki/George_Clooney\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Matthew_McConaughey\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Leonardo_DiCaprio\n",
      "/wiki/Casey_Affleck\n",
      "/wiki/Gary_Oldman\n",
      "/wiki/Christian_Bale\n",
      "/wiki/Joaquin_Phoenix\n",
      "/wiki/Chadwick_Boseman\n",
      "/wiki/Will_Smith\n",
      "/wiki/Brendan_Fraser\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Anthony_Andrews\n",
      "/wiki/Richard_Chamberlain\n",
      "/wiki/Ted_Danson\n",
      "/wiki/Dustin_Hoffman\n",
      "/wiki/James_Woods\n",
      "/wiki/Randy_Quaid\n",
      "/wiki/Michael_Caine\n",
      "/wiki/Stacy_Keach\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Beau_Bridges\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Ving_Rhames\n",
      "/wiki/Stanley_Tucci\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/James_Franco\n",
      "/wiki/Albert_Finney\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jonathan_Rhys_Meyers\n",
      "/wiki/Bill_Nighy\n",
      "/wiki/Jim_Broadbent\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Billy_Bob_Thornton\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Tom_Hiddleston\n",
      "/wiki/Ewan_McGregor\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Evan_Peters\n",
      "/wiki/Steven_Yeun\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Kyle_Chandler\n",
      "/wiki/Steven_Weber_(actor)\n",
      "/wiki/Richard_Dean_Anderson\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Robert_Patrick\n",
      "/wiki/Ben_Browder\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Ben_Browder\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Michael_C._Hall\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Edward_James_Olmos\n",
      "/wiki/Josh_Holloway\n",
      "/wiki/Stephen_Moyer\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Mads_Mikkelsen\n",
      "/wiki/Hugh_Dancy\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Bruce_Campbell\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Kyle_MacLachlan\n",
      "/wiki/Patrick_Stewart\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Network_or_Cable_Television_Series\n",
      "/wiki/Sam_Heughan\n",
      "/wiki/Bob_Odenkirk\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Streaming_Television_Series\n",
      "/wiki/Henry_Thomas\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Patrick_Stewart\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Christopher_Reeve\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/Ben_Kingsley\n",
      "/wiki/William_H._Macy\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Paul_Newman\n",
      "/wiki/Jeremy_Irons\n",
      "/wiki/Kevin_Kline\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Alexander_Skarsg%C3%A5rd\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Sam_Rockwell\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Sam_Elliott\n",
      "/wiki/Steven_Yeun\n"
     ]
    }
   ],
   "source": [
    "#You can use these rules to revise the code slightly to retrieve only the desired article\n",
    "#links by using the regular expression ^(/wiki/)((?!:).)*$\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "# here we specify the 3 conditions for the patterns note ?!: = don't contain colons\n",
    "for link in bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3476ee",
   "metadata": {},
   "source": [
    "The above a list of all article URLs that the Wikipedia article on Kevin Bacon links to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f01530",
   "metadata": {},
   "source": [
    "Of course, having a script that finds all article links in one, hardcoded Wikipedia arti‐\n",
    "cle, while interesting, is fairly useless in practice. You need to be able to take this code\n",
    "and transform it into something more like the following:\n",
    "* A single function, getLinks, that takes in a Wikipedia article URL of the\n",
    "form /wiki/<Article_Name> and returns a list of all linked article URLs in the\n",
    "same form.\n",
    "\n",
    "* A main function that calls getLinks with a starting article, chooses a random\n",
    "article link from the returned list, and calls getLinks again, until you stop the\n",
    "program or until no article links are found on the new page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a7788f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The only supported seed types are: None,\nint, float, str, bytes, and bytearray.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 90\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y155sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y155sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# DeprecationWarning: Seeding based on hashing is deprecated since Python 3.9 and will be removed in a subsequent version\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y155sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m random\u001b[39m.\u001b[39mseed(datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y155sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetLinks\u001b[39m(articleUrl):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y155sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     html \u001b[39m=\u001b[39m urlopen(\u001b[39m'\u001b[39m\u001b[39mhttp://en.wikipedia.org\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(articleUrl))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/random.py:160\u001b[0m, in \u001b[0;36mRandom.seed\u001b[0;34m(self, a, version)\u001b[0m\n\u001b[1;32m    157\u001b[0m     a \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m\u001b[39m.\u001b[39mfrom_bytes(a \u001b[39m+\u001b[39m _sha512(a)\u001b[39m.\u001b[39mdigest())\n\u001b[1;32m    159\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(a, (\u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m), \u001b[39mint\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe only supported seed types are: None,\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    161\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mint, float, str, bytes, and bytearray.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mseed(a)\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgauss_next \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: The only supported seed types are: None,\nint, float, str, bytes, and bytearray."
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "\n",
    "# DeprecationWarning: Seeding based on hashing is deprecated since Python 3.9 and will be removed in a subsequent version\n",
    "random.seed(datetime.datetime.now())\n",
    "\n",
    "def getLinks(articleUrl):\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(articleUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
    "\n",
    "links = getLinks('/wiki/Kevin_Bacon')\n",
    "while len(links) > 0:\n",
    "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
    "    print(newArticle)\n",
    "    links = getLinks(newArticle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b6657",
   "metadata": {},
   "source": [
    "### Crawling and Entire Site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e41c2",
   "metadata": {},
   "source": [
    "In the previous section, you took a random walk through a website, going from link\n",
    "to link. But what if you need to systematically catalog or search every page on a site?\n",
    "Crawling an entire site, especially a large one, is a memory-intensive process that is\n",
    "best suited to applications for which a database to store crawling results is readily\n",
    "available. However, you can explore the behavior of these types of applications\n",
    "without running them full-scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f0e12",
   "metadata": {},
   "source": [
    "The general approach to an exhaustive site crawl is to start with a top-level page (such\n",
    "as the home page), and search for a list of all internal links on that page. Every one of\n",
    "those links is then crawled, and additional lists of links are found on each one of\n",
    "them, triggering another round of crawling.\n",
    "\n",
    "If every page has 10 internal links,\n",
    "and a website is 5 pages deep (a fairly typical depth for a medium-size website), then the number of pages you need to crawl is 105\n",
    ", or 100,000 pages, before you can be\n",
    "sure that you’ve exhaustively covered the website.\n",
    "\n",
    "To avoid crawling the same page twice, it is extremely important that all internal links\n",
    "discovered are formatted consistently, and kept in a running set for easy lookups,\n",
    "while the program is running. A set is similar to a list, but elements do not have a\n",
    "specific order, and only unique elements will be stored, which is ideal for our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1be8269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "\n",
    "# Initially, getLinks is called with an empty URL\n",
    "def getLinks(pageUrl):\n",
    "    global pages\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "    getLinks('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f74c92",
   "metadata": {},
   "source": [
    "To show you the full effect of how this web crawling business works, I’ve relaxed the\n",
    "standards of what constitutes an internal link (from previous examples). Rather than\n",
    "limit the scraper to article pages, it looks for all links that begin with /wiki/, regardless\n",
    "of where they are on the page, and regardless of whether they contain colons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705974f",
   "metadata": {},
   "source": [
    "Initially getLinks is called with an empty URL. This is translated as “the front page\n",
    "of Wikipedia” as soon as the empty URL is prepended with http://en.wikipedia.org inside the function. Then, each link on the first page is iterated through and\n",
    "a check is made to see whether it is in the global set of pages (a set of pages that the\n",
    "script has encountered already). If not, it is added to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab5c53",
   "metadata": {},
   "source": [
    "if left running long enough, the preceding pro‐\n",
    "gram will almost certainly crash.\n",
    "Python has a default recursion limit (the number of times a pro‐\n",
    "gram can recursively call itself) of 1,000. Because Wikipedia’s net‐\n",
    "work of links is extremely large, this program will eventually hit\n",
    "that recursion limit and stop, unless you put in a recursion counter\n",
    "or something to prevent that from happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79347cd9",
   "metadata": {},
   "source": [
    "### Collecting Data Across an Entire Site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202e6a2",
   "metadata": {},
   "source": [
    "Web crawlers would be fairly boring if all they did was hop from one page to the\n",
    "other. To make them useful, you need to be able to do something on the page while\n",
    "you’re there. Let’s look at how to build a scraper that collects the title, the first para‐\n",
    "graph of content, and the link to edit the page (if available).\n",
    "\n",
    "For every link in the http://en.wikipedia.org when we click on each of the links on this page, we find the h1, then find the text of the first paragraph of each one and edit links - so the first print output would be from the https://en.wikipedia.org/wiki/Daytona_USA URL and title should be Daytona USA with text Daytona USA[a] is an arcade racing game developed by Sega AM2 and published by Sega in March 1994.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b86ef35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Page\n",
      "<p><i><b><a href=\"/wiki/Fallout_(video_game)\" title=\"Fallout (video game)\">Fallout: A Post Nuclear Role Playing Game</a></b></i> is a 1997 <a href=\"/wiki/Role-playing_video_game\" title=\"Role-playing video game\">role-playing video game</a> developed and published by <a href=\"/wiki/Interplay_Entertainment\" title=\"Interplay Entertainment\">Interplay Productions</a>. Set in a post-apocalyptic world in the mid–22nd century, it revolves around the player character seeking a replacement computer chip for their underground nuclear shelter's water supply system. The gameplay involves interacting with other survivors and engaging in <a href=\"/wiki/Timekeeping_in_games#Turn-based\" title=\"Timekeeping in games\">turn-based</a> combat. <i>Fallout</i> started development in 1994 as a <a href=\"/wiki/Game_engine\" title=\"Game engine\">game engine</a> designed by <a href=\"/wiki/Tim_Cain\" title=\"Tim Cain\">Tim Cain</a> <i>(pictured)</i>. It was originally based on <i><a href=\"/wiki/GURPS\" title=\"GURPS\">GURPS</a></i>, a <a href=\"/wiki/Role-playing_game_system\" title=\"Role-playing game system\">role-playing game system</a>, though the <a href=\"/wiki/Character_creation\" title=\"Character creation\">character-customization</a> scheme was changed after the <i>GURPS</i> license was terminated. <i>Fallout</i> drew artistic inspiration from <a href=\"/wiki/Atomic_Age_(design)\" title=\"Atomic Age (design)\">Atomic Age</a> media and is considered a spiritual successor to <i><a href=\"/wiki/Wasteland_(video_game)\" title=\"Wasteland (video game)\">Wasteland</a></i> (1988). The game was a critical and commercial success and spawned <a href=\"/wiki/Fallout_(series)\" title=\"Fallout (series)\">a successful series of sequels and spin-offs</a>. It has since been credited for renewing consumer interest in computer role-playing games. (<b><a href=\"/wiki/Fallout_(video_game)\" title=\"Fallout (video game)\">Full article...</a></b>)\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Main_Page\n",
      "Main Page\n",
      "<p><i><b><a href=\"/wiki/Fallout_(video_game)\" title=\"Fallout (video game)\">Fallout: A Post Nuclear Role Playing Game</a></b></i> is a 1997 <a href=\"/wiki/Role-playing_video_game\" title=\"Role-playing video game\">role-playing video game</a> developed and published by <a href=\"/wiki/Interplay_Entertainment\" title=\"Interplay Entertainment\">Interplay Productions</a>. Set in a post-apocalyptic world in the mid–22nd century, it revolves around the player character seeking a replacement computer chip for their underground nuclear shelter's water supply system. The gameplay involves interacting with other survivors and engaging in <a href=\"/wiki/Timekeeping_in_games#Turn-based\" title=\"Timekeeping in games\">turn-based</a> combat. <i>Fallout</i> started development in 1994 as a <a href=\"/wiki/Game_engine\" title=\"Game engine\">game engine</a> designed by <a href=\"/wiki/Tim_Cain\" title=\"Tim Cain\">Tim Cain</a> <i>(pictured)</i>. It was originally based on <i><a href=\"/wiki/GURPS\" title=\"GURPS\">GURPS</a></i>, a <a href=\"/wiki/Role-playing_game_system\" title=\"Role-playing game system\">role-playing game system</a>, though the <a href=\"/wiki/Character_creation\" title=\"Character creation\">character-customization</a> scheme was changed after the <i>GURPS</i> license was terminated. <i>Fallout</i> drew artistic inspiration from <a href=\"/wiki/Atomic_Age_(design)\" title=\"Atomic Age (design)\">Atomic Age</a> media and is considered a spiritual successor to <i><a href=\"/wiki/Wasteland_(video_game)\" title=\"Wasteland (video game)\">Wasteland</a></i> (1988). The game was a critical and commercial success and spawned <a href=\"/wiki/Fallout_(series)\" title=\"Fallout (series)\">a successful series of sequels and spin-offs</a>. It has since been credited for renewing consumer interest in computer role-playing games. (<b><a href=\"/wiki/Fallout_(video_game)\" title=\"Fallout (video game)\">Full article...</a></b>)\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Wikipedia:Contents\n",
      "Wikipedia:Contents\n",
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Portal:Current_events\n",
      "Portal:Current events\n",
      "<p><span class=\"noprint\" id=\"coordinates\"><a href=\"/wiki/Portal:Current_events/Edit_instructions\" title=\"Portal:Current events/Edit instructions\">Edit instructions</a></span>\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Special:Random\n",
      "Annie Murray\n",
      "<p><b>Annie Cargill Knight</b> (nee <b>Murray</b>; 10 April 1906 - 4 November 1996) was a Scottish nurse in the <a href=\"/wiki/Spanish_Civil_War\" title=\"Spanish Civil War\">Spanish Civil War</a> (1936-1939). Knight was the daughter of a tenant farmer and one of eight children. She became active in the <a href=\"/wiki/Communist_party\" title=\"Communist party\">Communist Party</a> after she finished her training as a nurse. She was one of the first British volunteers to arrive in Spain on the side of the <a href=\"/wiki/Spanish_Republican_government_in_exile\" title=\"Spanish Republican government in exile\">Spanish Republican Government</a> during the Spanish Civil War. \n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Wikipedia:About\n",
      "Wikipedia:About\n",
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Help:Contents\n",
      "Help:Contents\n",
      "<p>\n",
      "<style data-mw-deduplicate=\"TemplateStyles:r1012508797\">.mw-parser-output .helpContents-wrapper{display:flex;flex-wrap:wrap}.mw-parser-output .helpContents-header{flex:1 0 100%;padding-bottom:1em;border-bottom:1px solid #a2a9b1;text-align:center}.mw-parser-output .helpContents-section{position:relative;flex:1 0 50%;min-width:380px;box-sizing:border-box;margin:1em 0;padding-left:30px;padding-right:20px}.mw-parser-output .helpContents-section h2{font-size:21px;padding:0;margin-top:1em;margin-bottom:0.25em;line-height:1.5}.mw-parser-output .helpContents-section ul{margin-left:0;padding-left:0}.mw-parser-output .helpContents-icon{position:absolute;top:25px;left:0;line-height:1;opacity:0.7}.mw-parser-output .helpContents-additional-search{text-align:center;margin-bottom:1em}</style>\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Help:Introduction\n",
      "Help:Introduction\n",
      "<p><style data-mw-deduplicate=\"TemplateStyles:r1156591132\">.mw-parser-output .introtosingle__main{position:relative;box-sizing:border-box;box-shadow:2px 2px 2px #CCC;max-width:100%;overflow:hidden;border:1px solid black;margin:auto;padding-bottom:20px}.mw-parser-output .introtosingle__main p{margin-bottom:2.0em}.mw-parser-output .introtosingle__main-withbackground{background-image:url(\"https://upload.wikimedia.org/wikipedia/commons/d/d9/Wikipedia-logo-v2-o10.svg\");background-position:center -500px;background-repeat:no-repeat;background-size:auto 150%}.mw-parser-output .introtosingle__main-title{font-size:250%;line-height:150%;background:#777;color:#FFF;text-align:center;align-items:center;justify-content:center}.mw-parser-output .introtosingle__lead{background-color:#EEE;background-color:rgba(221,221,221,0.5);padding:30px 60px;margin-bottom:15px}.mw-parser-output .introtosingle__base{box-sizing:border-box;max-width:1100px;min-height:55px;margin:auto;padding:5px 20px;font-size:1.1em;background:#EEE;border:1px solid lightgrey}.mw-parser-output .introtosingle__columns{display:flex;flex-direction:row;flex-wrap:wrap-reverse;justify-content:center}.mw-parser-output .introtosingle__columns-left,.mw-parser-output .introtosingle__columns-left-noborder,.mw-parser-output .introtosingle__columns-right{display:inline-block;flex:1 1 0;align-self:flex-end;vertical-align:top;min-width:200px;max-width:300px;padding:10px}.mw-parser-output .introtosingle__columns-left{text-align:right;justify-content:right;border-right:solid 1px #ddd}.mw-parser-output .introtosingle__columns-left-noborder{text-align:right;justify-content:right}.mw-parser-output .introtosingle__columns-right{text-align:left;justify-content:left}@media screen and (min-width:1101px){.mw-parser-output .introtosingle__main{max-width:1100px}}</style>\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Wikipedia:Community_portal\n",
      "Wikipedia:Community portal\n",
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Special:RecentChanges\n",
      "Recent changes\n",
      "<p>This is a list of recent changes to Wikipedia.\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Wikipedia:File_upload_wizard\n",
      "Wikipedia:File upload wizard\n",
      "<p>Thank you for offering to contribute an image or other media file for use on Wikipedia. This wizard will guide you through a questionnaire prompting you for the appropriate copyright and sourcing information for each file. Please ensure you understand <a href=\"/wiki/Wikipedia:Copyrights\" title=\"Wikipedia:Copyrights\">copyright</a> and the <a href=\"/wiki/Wikipedia:Image_use_policy\" title=\"Wikipedia:Image use policy\">image use policy</a> before proceeding.\n",
      "</p>\n",
      "This page is missing something! Continuing.\n",
      "--------------------\n",
      "/wiki/Special:Search\n",
      "Search\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 100\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m                 pages\u001b[39m.\u001b[39madd(newPage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m                 getLinks(newPage)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m getLinks(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 100\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(newPage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pages\u001b[39m.\u001b[39madd(newPage)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m getLinks(newPage)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 100\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(newPage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pages\u001b[39m.\u001b[39madd(newPage)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m getLinks(newPage)\n",
      "    \u001b[0;31m[... skipping similar frames: getLinks at line 25 (8 times)]\u001b[0m\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 100\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(newPage)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m pages\u001b[39m.\u001b[39madd(newPage)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m getLinks(newPage)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 100\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(bs\u001b[39m.\u001b[39mh1\u001b[39m.\u001b[39mget_text()) \u001b[39m#find titles defined under h1\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(bs\u001b[39m.\u001b[39mfind(\u001b[39mid\u001b[39m \u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmw-content-text\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mp\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]) \u001b[39m#access 1st paragraph of text i.e. p[0]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(bs\u001b[39m.\u001b[39mfind(\u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mca-edit\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mattrs[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m#find edit links\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y201sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "pages = set()\n",
    "def getLinks(pageUrl):\n",
    "    global pages #global variable\n",
    "    html = urlopen('http://en.wikipedia.org{}'.format(pageUrl))\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    try:\n",
    "        print(bs.h1.get_text()) #find titles defined under h1\n",
    "        print(bs.find(id ='mw-content-text').find_all('p')[0]) #access 1st paragraph of text i.e. p[0]\n",
    "        print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) #find edit links\n",
    "    except AttributeError:\n",
    "        print('This page is missing something! Continuing.')\n",
    "\n",
    "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            if link.attrs['href'] not in pages:\n",
    "                #We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                print('-'*20)\n",
    "                print(newPage)\n",
    "                pages.add(newPage)\n",
    "                getLinks(newPage)\n",
    "getLinks('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a9893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f61928",
   "metadata": {},
   "source": [
    "The for loop in this program is essentially the same as it was in the original crawling\n",
    "program (with the addition of printed dashes for clarity, separating the printed con‐\n",
    "tent).\n",
    "\n",
    "Because you can never be entirely sure that all the data is on each page, each print\n",
    "statement is arranged in the order that it is likeliest to appear on the site. That is, the\n",
    "h1 title tag appears on every page (as far as I can tell, at any rate) so you attempt to get\n",
    "that data first. The text content appears on most pages (except for file pages), so that\n",
    "is the second piece of data retrieved. The Edit button appears only on pages in which\n",
    "both titles and text content already exist, but it does not appear on all of those pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6071b1f",
   "metadata": {},
   "source": [
    "#### Handling Redirects\n",
    "\n",
    "Redirects allow a web server to point one domain name or URL to a piece of content\n",
    "at a different location. There are two types of redirects:\n",
    "\n",
    "* Server-side redirects, where the URL is changed before the page is loaded\n",
    "* Client-side redirects, sometimes seen with a “You will be redirected in 10 sec‐\n",
    "onds” type of message, where the page loads before redirecting to the new one\n",
    "\n",
    "With server-side redirects, you usually don’t have to worry. If you’re using the urllib\n",
    "library with Python 3.x, it handles redirects automatically! If you’re using the requests\n",
    "library, make sure to set the allow-redirects flag to True:\n",
    "\n",
    "    r = requests.get('http://github.com', allow_redirects=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b464853",
   "metadata": {},
   "source": [
    "### Crawling Across the Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17960573",
   "metadata": {},
   "source": [
    "Before you start writing a crawler that follows all outbound links willy-nilly, you\n",
    "should ask yourself a few questions:\n",
    "* What data am I trying to gather? Can this be accomplished by scraping just a few\n",
    "predefined websites (almost always the easier option), or does my crawler need to\n",
    "be able to discover new websites I might not know about?\n",
    "* When my crawler reaches a particular website, will it immediately follow the next\n",
    "outbound link to a new website, or will it stick around for a while and drill down\n",
    "into the current website?\n",
    "* Are there any conditions under which I would not want to scrape a particular\n",
    "site? Am I interested in non-English content?\n",
    "* How am I protecting myself against legal action if my web crawler catches the\n",
    "attention of a webmaster on one of the sites it runs across? (Check out Chap‐\n",
    "ter 18 for more information on this subject.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf5071a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "pages = set()\n",
    "#random.seed(datetime.datetime.now())\n",
    "\n",
    "#Retrieves a list of all Internal links found on a page\n",
    "def getInternalLinks(bs, includeUrl):\n",
    "    includeUrl = '{}://{}'.format(urlparse(includeUrl).scheme,urlparse(includeUrl).netloc)\n",
    "    internalLinks = []\n",
    "#Finds all links that begin with a \"/\"\n",
    "    for link in bs.find_all('a',href=re.compile('^(/|.*'+includeUrl+')')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in internalLinks:\n",
    "                if(link.attrs['href'].startswith('/')):\n",
    "                    internalLinks.append(includeUrl+link.attrs['href'])\n",
    "                else:\n",
    "                    internalLinks.append(link.attrs['href'])\n",
    "    return internalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf53094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieves a list of all external links found on a page\n",
    "def getExternalLinks(bs, excludeUrl):\n",
    "    externalLinks = []\n",
    "    #Finds all links that start with \"http\" that do\n",
    "    #not contain the current URL\n",
    "    for link in bs.find_all('a',href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')):\n",
    "        if link.attrs['href'] is not None:\n",
    "            if link.attrs['href'] not in externalLinks:\n",
    "                externalLinks.append(link.attrs['href'])\n",
    "        return externalLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9f9592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomExternalLink(startingPage):\n",
    "    html = urlopen(startingPage)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    externalLinks = getExternalLinks(bs,urlparse(startingPage).netloc)\n",
    "    if len(externalLinks) == 0:\n",
    "        print('No external links, looking around the site for one')\n",
    "        domain = '{}://{}'.format(urlparse(startingPage).scheme,urlparse(startingPage).netloc)\n",
    "        internalLinks = getInternalLinks(bs, domain)\n",
    "        return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)])\n",
    "    else:\n",
    "        return externalLinks[random.randint(0, len(externalLinks)-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39fc5def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def followExternalOnly(startingSite):\n",
    "    externalLink = getRandomExternalLink(startingSite)\n",
    "    print('Random external link is: {}'.format(externalLink))\n",
    "    followExternalOnly(externalLink)\n",
    "    followExternalOnly('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae579e73",
   "metadata": {},
   "source": [
    "External links are not always guaranteed to be found on the first page of a website. To\n",
    "find external links in this case, a method similar to the one used in the previous\n",
    "crawling example is employed to recursively drill down into a website until it finds an\n",
    "external link.\n",
    "\n",
    "Any external links > yes > return random external link \n",
    "Any external links > no > go to an internal link on page > get all external links on that page > any external links > yes > return random external link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9c08c",
   "metadata": {},
   "source": [
    "For example, if an external link is not found anywhere\n",
    "on a site that this crawler encounters (unlikely, but it’s bound to\n",
    "happen at some point if you run it for long enough), this program\n",
    "will keep running until it hits Python’s recursion limit.\n",
    "\n",
    "One easy way to increase the robustness of this crawler would be to\n",
    "combine it with the connection exception-handling code in Chap‐\n",
    "ter 1.\n",
    "\n",
    "The nice thing about breaking up tasks into simple functions such as “find all external\n",
    "links on this page” is that the code can later be easily refactored to perform a different\n",
    "crawling task. For example, if your goal is to crawl an entire site for external links,\n",
    "and make a note of each one, you can add the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b90365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.oreilly.com\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 112\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             getAllExternalLinks(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m allIntLinks\u001b[39m.\u001b[39madd(\u001b[39m'\u001b[39m\u001b[39mhttp://oreilly.com\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m getAllExternalLinks(\u001b[39m'\u001b[39m\u001b[39mhttp://oreilly.com\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 112\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m link \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allIntLinks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     allIntLinks\u001b[39m.\u001b[39madd(link)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     getAllExternalLinks(link)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 112\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m link \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allIntLinks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     allIntLinks\u001b[39m.\u001b[39madd(link)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     getAllExternalLinks(link)\n",
      "    \u001b[0;31m[... skipping similar frames: getAllExternalLinks at line 19 (2 times)]\u001b[0m\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 112\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mif\u001b[39;00m link \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allIntLinks:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     allIntLinks\u001b[39m.\u001b[39madd(link)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     getAllExternalLinks(link)\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 112\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetAllExternalLinks\u001b[39m(siteUrl):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     html \u001b[39m=\u001b[39m urlopen(siteUrl)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     domain \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m://\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(urlparse(siteUrl)\u001b[39m.\u001b[39mscheme, urlparse(siteUrl)\u001b[39m.\u001b[39mnetloc)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y216sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     bs \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[39m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[39mreturn\u001b[39;00m opener\u001b[39m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[39m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m'\u001b[39m\u001b[39murllib.Request\u001b[39m\u001b[39m'\u001b[39m, req\u001b[39m.\u001b[39mfull_url, req\u001b[39m.\u001b[39mdata, req\u001b[39m.\u001b[39mheaders, req\u001b[39m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_open(req, data)\n\u001b[1;32m    521\u001b[0m \u001b[39m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[39m=\u001b[39m protocol\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_response\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[39m=\u001b[39m req\u001b[39m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_open, protocol, protocol \u001b[39m+\u001b[39m\n\u001b[1;32m    537\u001b[0m                           \u001b[39m'\u001b[39m\u001b[39m_open\u001b[39m\u001b[39m'\u001b[39m, req)\n\u001b[1;32m    538\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mfor\u001b[39;00m handler \u001b[39min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1377\u001b[0m, in \u001b[0;36mHTTPHandler.http_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhttp_open\u001b[39m(\u001b[39mself\u001b[39m, req):\n\u001b[0;32m-> 1377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_open(http\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mHTTPConnection, req)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         h\u001b[39m.\u001b[39mrequest(req\u001b[39m.\u001b[39mget_method(), req\u001b[39m.\u001b[39mselector, req\u001b[39m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[39m=\u001b[39mreq\u001b[39m.\u001b[39mhas_header(\u001b[39m'\u001b[39m\u001b[39mTransfer-encoding\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m err: \u001b[39m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[39mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1286\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\u001b[39mself\u001b[39m, method, url, body\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, headers\u001b[39m=\u001b[39m{}, \u001b[39m*\u001b[39m,\n\u001b[1;32m   1284\u001b[0m             encode_chunked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1285\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1286\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1332\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(body, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1329\u001b[0m     \u001b[39m# RFC 2616 Section 3.7.1 says that text default has a\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[39m# default charset of iso-8859-1.\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m     body \u001b[39m=\u001b[39m _encode(body, \u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1332\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendheaders(body, encode_chunked\u001b[39m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1281\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_output(message_body, encode_chunked\u001b[39m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1041\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1039\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer)\n\u001b[1;32m   1040\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(msg)\n\u001b[1;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m message_body \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m \n\u001b[1;32m   1045\u001b[0m     \u001b[39m# create a consistent interface to message_body\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(message_body, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m   1047\u001b[0m         \u001b[39m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m         \u001b[39m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m         \u001b[39m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:979\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    978\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_open:\n\u001b[0;32m--> 979\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnect()\n\u001b[1;32m    980\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m         \u001b[39mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:945\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[39;00m\n\u001b[1;32m    944\u001b[0m sys\u001b[39m.\u001b[39maudit(\u001b[39m\"\u001b[39m\u001b[39mhttp.client.connect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport)\n\u001b[0;32m--> 945\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection(\n\u001b[1;32m    946\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mport), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_address)\n\u001b[1;32m    947\u001b[0m \u001b[39m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[39mif\u001b[39;00m source_address:\n\u001b[1;32m    835\u001b[0m     sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[0;32m--> 836\u001b[0m sock\u001b[39m.\u001b[39mconnect(sa)\n\u001b[1;32m    837\u001b[0m \u001b[39m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[1;32m    838\u001b[0m exceptions\u001b[39m.\u001b[39mclear()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Collects a list of all external URLs found on the site\n",
    "allExtLinks = set()\n",
    "allIntLinks = set()\n",
    "\n",
    "def getAllExternalLinks(siteUrl):\n",
    "    html = urlopen(siteUrl)\n",
    "    domain = '{}://{}'.format(urlparse(siteUrl).scheme, urlparse(siteUrl).netloc)\n",
    "    bs = BeautifulSoup(html, 'html.parser')\n",
    "    internalLinks = getInternalLinks(bs, domain)\n",
    "    externalLinks = getExternalLinks(bs, domain)\n",
    "    \n",
    "    for link in externalLinks:\n",
    "        if link not in allExtLinks:\n",
    "            allExtLinks.add(link)\n",
    "            print(link)\n",
    "    for link in internalLinks:\n",
    "        if link not in allIntLinks:\n",
    "            allIntLinks.add(link)\n",
    "            getAllExternalLinks(link)\n",
    "\n",
    "allIntLinks.add('http://oreilly.com')\n",
    "getAllExternalLinks('http://oreilly.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54695332",
   "metadata": {},
   "source": [
    "This code can be thought of as two loops—one gathering internal links, one gathering\n",
    "external links—working in conjunction with each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf6f156",
   "metadata": {},
   "source": [
    "# 4. Writing Web Crawlers  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e097a",
   "metadata": {},
   "source": [
    "You may be asked to collect news articles or blog posts from a variety of websites,\n",
    "each with different templates and layouts. One website’s h1 tag contains the title of the\n",
    "article, another’s h1 tag contains the title of the website itself, and the article title is in \n",
    "    \n",
    "    <span id=\"title\">\n",
    "\n",
    "You may need flexible control over which websites are scraped and how they’re scra‐\n",
    "ped, and a way to quickly add new websites or modify existing ones, as fast as possi‐\n",
    "ble, without writing multiple lines of code.\n",
    "\n",
    "You may be asked to scrape product prices from different websites, with the ultimate\n",
    "aim of comparing prices for the same product. Perhaps these prices are in different\n",
    "currencies, and perhaps you’ll also need to combine this with external data from\n",
    "some other nonweb source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99584b",
   "metadata": {},
   "source": [
    "## Planning and Defining Objects\n",
    "\n",
    "If you want to collect\n",
    "product data, you may first look at a clothing store and decide that each product you\n",
    "scrape needs to have the following fields:\n",
    "\n",
    "* product name\n",
    "* price\n",
    "* description\n",
    "* sizes\n",
    "* colours\n",
    "* fabric type\n",
    "* customer rating\n",
    "* item SKU - from another website\n",
    "\n",
    "Although clothing may be a great start, you also want to make sure you can extend\n",
    "this crawler to other types of products. You start perusing product sections of other\n",
    "websites and decide you also need to collect this information:\n",
    "\n",
    "* hardcover/paperback\n",
    "* matt/glossy paint\n",
    "* number customer reviews\n",
    "* link to manurfacturer\n",
    "\n",
    "Clearly, this is an unsustainable approach. Simply adding attributes to your product\n",
    "type every time you see a new piece of information on a website will lead to far too\n",
    "many fields to keep track of. \n",
    "\n",
    "Not only that, but every time you scrape a new website,\n",
    "you’ll be forced to perform a detailed analysis of the fields the website has and the\n",
    "fields you’ve accumulated so far, and potentially add new fields (modifying your\n",
    "Python object type and your database structure).\n",
    "\n",
    "You need to limit the amount of information that you need to track to make it achievable.\n",
    "\n",
    "Perhaps what you really want to do is compare product prices among multiple stores\n",
    "and track those product prices over time. In this case, you need enough information\n",
    "to uniquely identify the product, and that’s it:\n",
    "\n",
    "* product title\n",
    "* manurfacturer\n",
    "* product ID number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc7a44",
   "metadata": {},
   "source": [
    "## Dealing with different website layouts\n",
    "\n",
    "One of the most impressive feats of a search engine such as Google is that it manages\n",
    "to extract relevant and useful data from a variety of websites, having no upfront\n",
    "knowledge about the website structure itself.\n",
    "\n",
    "Fortunately, in most cases of web crawling, you’re not looking to collect data from\n",
    "sites you’ve never seen before, but from a few, or a few dozen, websites that are preselected by a human. \n",
    "\n",
    "The most obvious approach is to write a separate web crawler or page parser for each\n",
    "website. Each might take in a URL, string, or BeautifulSoup object, and return a\n",
    "Python object for the thing that was scraped.\n",
    "\n",
    "The following is an example of a Content class (representing a piece of content on a\n",
    "website, such as a news article) and two scraper functions that take in a Beauti\n",
    "fulSoup object and return an instance of Content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1752204c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 118\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# url links do not exist anymore on this site\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m content \u001b[39m=\u001b[39m scrapeBrookings(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTitle: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(content\u001b[39m.\u001b[39mtitle))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mURL: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(content\u001b[39m.\u001b[39murl))\n",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 118\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m bs \u001b[39m=\u001b[39m getPage(url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m title \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtext\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m body \u001b[39m=\u001b[39m bs\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m\"\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mpost-body\u001b[39m\u001b[39m\"\u001b[39m})\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y225sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m Content(url, title, body)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "def getPage(url):\n",
    "    req = requests.get(url)\n",
    "    return BeautifulSoup(req.text, 'html.parser')\n",
    "\n",
    "def scrapeNYTimes(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    lines = bs.find_all(\"p\", {\"class\":\"story-content\"})\n",
    "    body = '\\n'.join([line.text for line in lines])\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = getPage(url)\n",
    "    title = bs.find(\"h1\").text\n",
    "    body = bs.find(\"div\",{\"class\",\"post-body\"}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "# url links do not exist anymore on this site\n",
    "url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'\n",
    " \n",
    "content = scrapeBrookings(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n",
    "# url links do not exist anymore on this site\n",
    "url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'\n",
    "content = scrapeNYTimes(url)\n",
    "print('Title: {}'.format(content.title))\n",
    "print('URL: {}\\n'.format(content.url))\n",
    "print(content.body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4be465",
   "metadata": {},
   "source": [
    "As you start to add scraper functions for additional news sites, you might notice a\n",
    "pattern forming. Every site’s parsing function does essentially the same thing:\n",
    "\n",
    "* Selects the title element and extracts the text for the title\n",
    "* Selects the main content of the article\n",
    "* Selects other content items as needed\n",
    "* Returns a Content object instantiated with the strings found previously\n",
    "\n",
    "To make things even more convenient, rather than dealing with all of these tag argu‐\n",
    "ments and key/value pairs, you can use the BeautifulSoup select function with a sin‐\n",
    "gle string CSS selector for each piece of information you want to collect and put all of\n",
    "these selectors in a dictionary object:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e8f25",
   "metadata": {},
   "source": [
    "        class Content:\n",
    "            \"\"\"\n",
    "            Common base class for all articles/pages\n",
    "            \"\"\"\n",
    "\n",
    "        def __init__(self, url, title, body):\n",
    "            self.url = url\n",
    "            self.title = title\n",
    "            self.body = body\n",
    "\n",
    "        def print(self):\n",
    "            \"\"\"\n",
    "            Flexible printing function controls output\n",
    "            \"\"\"\n",
    "            print(\"URL: {}\".format(self.url))\n",
    "            print(\"TITLE: {}\".format(self.title))\n",
    "            print(\"BODY:\\n{}\".format(self.body))\n",
    "\n",
    "        class Website:\n",
    "        \"\"\"\n",
    "        Contains information about website structure\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, name, url, titleTag, bodyTag):\n",
    "            self.name = name\n",
    "            self.url = url\n",
    "            self.titleTag = titleTag\n",
    "            self.bodyTag = bodyTag\n",
    "\n",
    "Note that the Website class does not store information collected from the individual\n",
    "pages themselves, but stores instructions about how to collect that data.  It simply stores the string tag h1 that indicates where\n",
    "the titles can be found. This is why the class is called Website (the information here\n",
    "pertains to the entire website) and not Content (which contains information from\n",
    "just a single page).\n",
    "\n",
    "Using these Content and Website classes you can then write a Crawler to scrape the\n",
    "title and content of any URL that is provided for a given web page from a given web‐\n",
    "site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3cfe40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "        \n",
    "    def safeGet(self, pageObj, selector):\n",
    "        \"\"\"\n",
    "        Utility function used to get a content string from a\n",
    "        Beautiful Soup object and a selector. Returns an empty\n",
    "        string if no object is found for the given selector\n",
    "        \"\"\"\n",
    "        selectedElems = pageObj.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    \n",
    "    def parse(self, site, url):\n",
    "        \"\"\"\n",
    "        Extract content from a given page URL\n",
    "        \"\"\"\n",
    "        bs = self.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = self.safeGet(bs, site.titleTag)\n",
    "            body = self.safeGet(bs, site.bodyTag)\n",
    "            if title != '' and body != '':\n",
    "                content = Content(url, title, body)\n",
    "                content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d2a8690",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jordanstevens/Documents/Python/Web Scraping in Python/Web Scraping Book 2nd Edition/Web Scraping.ipynb Cell 122\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y315sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m websites \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y315sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m siteData:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y315sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     websites\u001b[39m.\u001b[39mappend(websites(row[\u001b[39m0\u001b[39m], row[\u001b[39m1\u001b[39m], row[\u001b[39m2\u001b[39m], row[\u001b[39m3\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y315sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m crawler\u001b[39m.\u001b[39mparse(websites[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mhttp://shop.oreilly.com/product/0636920028154.do\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jordanstevens/Documents/Python/Web%20Scraping%20in%20Python/Web%20Scraping%20Book%202nd%20Edition/Web%20Scraping.ipynb#Y315sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m crawler\u001b[39m.\u001b[39mparse(websites[\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mhttp://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# And here’s the code that defines the website objects and kicks off the process:\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "    ['O\\'Reilly Media', 'http://oreilly.com','h1', 'section#product-description'],\n",
    "    ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "    ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "    ['New York Times', 'http://nytimes.com','h1', 'p.story-content']\n",
    "]\n",
    "\n",
    "websites = []\n",
    "\n",
    "for row in siteData:\n",
    "    websites.append(websites(row[0], row[1], row[2], row[3]))\n",
    "    \n",
    "crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')\n",
    "crawler.parse(websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')\n",
    "crawler.parse(websites[2], 'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')\n",
    "crawler.parse(websites[3], 'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d5286a",
   "metadata": {},
   "source": [
    "While this new method might not seem remarkably simpler than writing a new\n",
    "Python function for each new website at first glance, imagine what happens when you\n",
    "go from a system with 4 website sources to a system with 20 or 200 sources.\n",
    "\n",
    "Each list of strings is relatively easy to write. It doesn’t take up much space. It can be\n",
    "loaded from a database or a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79f3e3",
   "metadata": {},
   "source": [
    "## Structuring Crawlers\n",
    "\n",
    "Creating flexible and modifiable website layout types doesn’t do much good if you\n",
    "still have to locate each link you want to scrape by hand. The previous chapter\n",
    "showed various methods of crawling through websites and finding new pages in an\n",
    "automated way.\n",
    "\n",
    "\n",
    "This section shows how to incorporate these methods into a well-structured and\n",
    "expandable website crawler that can gather links and discover data in an automated\n",
    "way. I "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93f67d",
   "metadata": {},
   "source": [
    "### Crawling Sites Through Search\n",
    "\n",
    "One of the easiest ways to crawl a website is via the same method that humans do:\n",
    "using the search bar\n",
    "\n",
    "* Most sites retrieve a list of search results for a particular topic by passing that\n",
    "topic as a string through a parameter in the URL. For example: http://exam\n",
    "ple.com?search=myTopic. The first part of this URL can be saved as a property\n",
    "of the Website object, and the topic can simply be appended to it.\n",
    "\n",
    "* After searching, most sites present the resulting pages as an easily identifiable list\n",
    "of links, usually with a convenient surrounding tag such as <span\n",
    "class=\"result\">, the exact format of which can also be stored as a property of\n",
    "the Website object.\n",
    "\n",
    "* Each result link is either a relative URL (e.g., /articles/page.html) or an absolute\n",
    "URL (e.g., http://example.com/articles/page.html). Whether or not you are expect‐\n",
    "ing an absolute or relative URL can be stored as a property of the Website object.\n",
    "\n",
    "* After you’ve located and normalized the URLs on the search page, you’ve suc‐\n",
    "cessfully reduced the problem to the example in the previous section—extracting\n",
    "data from a page, given a website format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09f8558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"Common base class for all articles/pages\"\"\"\n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        self.url = url\n",
    "\n",
    "def print(self):\n",
    "    \"\"\"\n",
    "    Flexible printing function controls output\n",
    "    \"\"\"\n",
    "    print(\"New article found for topic: {}\".format(self.topic))\n",
    "    print(\"TITLE: {}\".format(self.title))\n",
    "    print(\"BODY:\\n{}\".format(self.body))\n",
    "    print(\"URL: {}\".format(self.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d13b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"Contains information about website structure\"\"\"\n",
    "    def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl # defines where you should go to get search results if you append the topic you are looking for.\n",
    "        self.resultListing = resultListing # holds information about each result\n",
    "        self.resultUrl = resultUrl #defines the tag inside this box that will give you the exact URL for the result\n",
    "        self.absoluteUrl=absoluteUrl #  tells you whether these search results are absolute or relative URLs.\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64743a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Crawler:\n",
    "    def getPage(self, url):\n",
    "        try:\n",
    "            req = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "        return BeautifulSoup(req.text, 'html.parser')\n",
    "    \n",
    "    def safeGet(self, pageObj, selector):\n",
    "        childObj = pageObj.select(selector)\n",
    "        if childObj is not None and len(childObj) > 0:\n",
    "            return childObj[0].get_text()\n",
    "        return \"\"\n",
    "\n",
    "def search(self, topic, site):\n",
    "    \"\"\"\n",
    "    Searches a given website for a given topic and records all pages found\n",
    "    \"\"\"\n",
    "    bs = self.getPage(site.searchUrl + topic)\n",
    "    searchResults = bs.select(site.resultListing)\n",
    "    for result in searchResults:\n",
    "        url = result.select(site.resultUrl)[0].attrs[\"href\"]\n",
    "        # Check to see whether it's a relative or an absolute URL\n",
    "        if(site.absoluteUrl):\n",
    "            bs = self.getPage(url)\n",
    "        else:\n",
    "            bs = self.getPage(site.url + url)\n",
    "        if bs is None:\n",
    "            print(\"Something was wrong with that page or URL. Skipping!\")\n",
    "            return\n",
    "        title = self.safeGet(bs, site.titleTag)\n",
    "        body = self.safeGet(bs, site.bodyTag)\n",
    "        if title != '' and body != '':\n",
    "            content = Content(topic, title, body, url)\n",
    "            content.print()\n",
    "\n",
    "\n",
    "crawler = Crawler()\n",
    "\n",
    "siteData = [\n",
    "['O\\'Reilly Media', 'http://oreilly.com','https://ssearch.oreilly.com/?q=article.product-result', 'p.title a', True, 'h1', 'section#product-description'],\n",
    "['Reuters', 'http://reuters.com','http://www.reuters.com/search/news?blob=div.search-result-content','h3.search-result-title a',False, 'h1', 'div.StandardArticleBody_body_1gnLA'],\n",
    "['Brookings', 'http://www.brookings.edu','https://www.brookings.edu/?s=div.list-contentarticle', 'h4.title a', True, 'h1','div.post-body']\n",
    "]\n",
    "\n",
    "sites = []\n",
    "for row in siteData:\n",
    "    sites.append(Website(row[0], row[1], row[2],row[3], row[4], row[5], row[6], row[7]))\n",
    "    \n",
    "topics = ['python', 'data science']\n",
    "for topic in topics:\n",
    "    print(\"GETTING INFO ABOUT: \" + topic)\n",
    "    for targetSite in sites:\n",
    "        crawler.search(topic, targetSite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055c5fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d83f973e",
   "metadata": {},
   "source": [
    "# 5. Scarpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554db1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca8d12c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab7b164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390d8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "819c56c2",
   "metadata": {},
   "source": [
    "# 10. Crawling Through Forms and Logins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8828cdb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c06924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af93a678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed0ef4a1",
   "metadata": {},
   "source": [
    "# 13. Image Processing and Text Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114dfaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97468b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b3d654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d8a1720",
   "metadata": {},
   "source": [
    "# 14. Avoiding Scraping Traps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605a1510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71937c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b29b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5c7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5451ce3",
   "metadata": {},
   "source": [
    "# 15. Testing Your Website with Scrapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16392d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371eb0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da41b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf3ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dbba37d",
   "metadata": {},
   "source": [
    "# 16. Web Crawling in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc346f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0130d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa69d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647682d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19e7778",
   "metadata": {},
   "source": [
    "# 17. Scraping Remotely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252c1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46312ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
